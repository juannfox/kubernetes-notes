# CKA 2022 exam notes

Some **Spanish** Kubernetes notes related to CKA training (dated 2022). The original content was written on a personal Confluence site and exporting it to Markdown didn't work very well, so I apologize for the bad Markdown syntax practices.

Notas de Kubernetes para CKA en Espa√±ol (2022). El contenido original estaba escrito en un sitio de Confluence personal y exportarlo a Markdown no funcion√≥ muy bien, por lo que me disculpo por las malas pr√°cticas de sintaxis de Markdown.

* 1 [Conceptos](#:info:-Conceptos)
  * 1.1 [Generales](#generales)
    * 1.1.1 [Cl√∫ster](#Cl√∫ster)
    * 1.1.2 [Recursos](#recursos)
    * 1.1.3 [Enfoque Imperativo](#enfoque-imperativo)
    * 1.1.4 [Enfoque Declarativo](#enfoque-declarativo)
    * 1.1.5 [Manifiesto](#manifiesto)
      * 1.1.5.1 [Estados de un manifiesto](#estados-de-un-manifiesto)
    * 1.1.6 [Label](#label)
      * 1.1.6.1 [Label Selector](#label-selector)
        * 1.1.6.1.1 [Queries con Label Selector](#queries-con-label-selector)
    * 1.1.7 [Annotation](#annotation)
  * 1.2 [Lenguajes](#[hardBreak]Lenguajes)
    * 1.2.1 [YALM](#yalm)
      * 1.2.1.1 [Pares clave-valor](#pares-clave-valor)
      * 1.2.1.2 [Array](#array)
      * 1.2.1.3 [Map o Diccionario](#map-o-diccionario)
    * 1.2.2 [JSON (JavaScript Object Notation)](#JSON-(JavaScript-Object-Notation))
      * 1.2.2.1 [Queries (JSONPath)](#Queries-(JSONPath))
      * 1.2.2.2 [Array](#Array.1)
      * 1.2.2.3 [Map o Diccionario](#Map-o-Diccionario.1)
      * 1.2.2.4 [JSONPath en K8s](#jsonpath-en-k8s)
        * 1.2.2.4.1 [Operaciones complejas - Columnas y orden](#operaciones-complejas---columnas-y-orden)
        * 1.2.2.4.2 [Queries condicionales](#queries-condicionales)
  * 1.3 [Master components](#master-components)
    * 1.3.1 [Kube-APIServer](#kube-apiserver)
    * 1.3.2 [ETCD](#etcd)
      * 1.3.2.1 [Funcionamiento](#funcionamiento)
      * 1.3.2.2 [Alta disponibilidad](#alta-disponibilidad)
      * 1.3.2.3 [Uso cl√°sico](#Uso-cl√°sico)
      * 1.3.2.4 [Uso en Kubernetes](#uso-en-kubernetes)
    * 1.3.3 [Kube Controller Manager](#kube-controller-manager)
      * 1.3.3.1 [Ejemplos de controllers](#ejemplos-de-controllers)
      * 1.3.3.2 [Cloud controller manager](#cloud-controller-manager)
    * 1.3.4 [Kube-Scheduler](#kube-scheduler)
      * 1.3.4.1 [Criterios de decisi√≥n](#Criterios-de-decisi√≥n)
      * 1.3.4.2 [Custom-Scheduler (Scheduler personalizado)](#Custom-Scheduler-(Scheduler-personalizado))
    * 1.3.5 [Container Runtime Interface (CRI)](#Container-Runtime-Interface-(CRI))
  * 1.4 [Otros componentes](#otros-componentes)
    * 1.4.1 [Namespace](#namespace)
      * 1.4.1.1 [ResourceQuota](#resourcequota)
    * 1.4.2 [Container](#container)
      * 1.4.2.1 [Imagen](#imagen)
      * 1.4.2.2 [Comandos y argumentos](#comandos-y-argumentos)
    * 1.4.3 [Pod](#pod)
      * 1.4.3.1 [MultiContainer](#multicontainer)
      * 1.4.3.2 [InitContainers](#initcontainers)
      * 1.4.3.3 [Afinidad](#afinidad)
        * 1.4.3.3.1 [Taints and tolerations](#taints-and-tolerations)
        * 1.4.3.3.2 [Node Selectors](#node-selectors)
        * 1.4.3.3.3 [Node Affinity and Anti-Affinity](#[hardBreak]Node-Affinity-and-Anti-Affinity)
        * 1.4.3.3.4 [Combinaci√≥n de Node Affinity y Taints/Tolerations](#[hardBreak]Combinaci√≥n-de-Node-Affinity-y-Taints/Tolerations)
      * 1.4.3.4 [Resources and Requests](#resources-and-requests)
        * 1.4.3.4.1 [Equivalencias de unidades](#equivalencias-de-unidades)
        * 1.4.3.4.2 [Acciones de l√≠mites:](#Acciones-de-l√≠mites:)
        * 1.4.3.4.3 [Valores por defecto](#valores-por-defecto)
      * 1.4.3.5 [Variables de entorno](#variables-de-entorno)
    * 1.4.4 [Node](#node)
      * 1.4.4.1 [Node-Components](#node-components)
      * 1.4.4.2 [Kubelet](#kubelet)
        * 1.4.4.2.1 [Kube-Proxy](#kube-proxy)
      * 1.4.4.3 [Static Pods](#static-pods)
    * 1.4.5 [Replica Set](#replica-set)
    * 1.4.6 [Deployment](#deployment)
      * 1.4.6.1 [Tipos de despliegue](#tipos-de-despliegue)
        * 1.4.6.1.1 [Rolling Update](#rolling-update)
        * 1.4.6.1.2 [Recreate](#recreate)
    * 1.4.7 [Service](#service)
      * 1.4.7.1 [Tipos de servicio](#tipos-de-servicio)
        * 1.4.7.1.1 [LoadBalancer](#loadbalancer)
        * 1.4.7.1.2 [NodePort](#nodeport)
        * 1.4.7.1.3 [ClusterIp](#clusterip)
        * 1.4.7.1.4 [Headless (ac√©falo)](#Headless-(ac√©falo))
      * 1.4.7.2 [Especificaciones](#especificaciones)
    * 1.4.8 [DaemonSet](#daemonset)
    * 1.4.9 [StatefulSet](#statefulset)
    * 1.4.10 [ConfigMap](#configmap)
    * 1.4.11 [Secreto](#secreto)
      * 1.4.11.1 [Niveles de seguridad](#niveles-de-seguridad)
      * 1.4.11.2 [Codificar/decodificar en Base 64](#Codificar/decodificar-en-Base-64)
* 2 [![:mag:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f50d.png) Observabilidad](#üîç-Observabilidad)
  * 2.1 [Monitoreo](#monitoreo)
    * 2.1.1 [Metric Server (Ex Heapster)](#Metric-Server-(Ex-Heapster))
      * 2.1.1.1 [Comandos](#comandos)
  * 2.2 [Logging](#logging)
* 3 [![:hammer:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f528.png) Mantenimiento del cl√∫ster](#üî®-Mantenimiento-del-cl√∫ster)
  * 3.1 [Mantenimiento de nodos](#mantenimiento-de-nodos)
    * 3.1.1 [Operaciones](#operaciones)
      * 3.1.1.1 [Drain](#drain)
      * 3.1.1.2 [Cordon](#cordon)
* 4 [![:cd:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4bf.png) Actualizaci√≥n del cl√∫ster](#üíø-Actualizaci√≥n-del-cl√∫ster)
  * 4.1 [Versionado de Kubernetes](#versionado-de-kubernetes)
  * 4.2 [Planeamiento previo](#planeamiento-previo)
  * 4.3 [C√≥mo actualizar](#C√≥mo-actualizar)
    * 4.3.1 [Actualizar con Kubeadm](#actualizar-con-kubeadm)
      * 4.3.1.1 [Vista previa](#vista-previa)
      * 4.3.1.2 [Comandos](#Comandos.1)
* 5 [![:floppy_disk:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4be.png) Respaldo/Backup del cl√∫ser](#üíæ-Respaldo/Backup-del-cl√∫ser)
  * 5.1 [Respaldo de Manifiestos](#respaldo-de-manifiestos)
  * 5.2 [Respaldo de ETCD](#respaldo-de-etcd)
* 6 [![:key:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f511.png) Seguridad](#üîë-Seguridad)
  * 6.1 [Nodos](#nodos)
  * 6.2 [Comunicaci√≥n entre componentes](#Comunicaci√≥n-entre-componentes)
  * 6.3 [Mecanismos de seguridad en un cl√∫ster](#Mecanismos-de-seguridad-en-un-cl√∫ster)
    * 6.3.1 [Autenticaci√≥n/Authentication](#Autenticaci√≥n/Authentication)
    * 6.3.2 [Autorizaci√≥n/Authorization](#Autorizaci√≥n/Authorization)
      * 6.3.2.1 [Mecanismos de autorizaci√≥n](#Mecanismos-de-autorizaci√≥n)
      * 6.3.2.2 [ABAP](#abap)
      * 6.3.2.3 [RBAC](#rbac)
        * 6.3.2.3.1 [Comandos](#Comandos.2)
        * 6.3.2.3.2 [Role definition](#role-definition)
        * 6.3.2.3.3 [Role binding](#role-binding)
        * 6.3.2.3.4 [ClusterRole](#clusterrole)
        * 6.3.2.3.5 [Cluster Role binding](#cluster-role-binding)
    * 6.3.3 [Service Account](#service-account)
      * 6.3.3.1 [Comandos](#Comandos.3)
  * 6.4 [Complementario: Certificados TLS](#Complementario:-Certificados-TLS)
    * 6.4.1 [Encripci√≥n sim√©trica](#Encripci√≥n-sim√©trica)
    * 6.4.2 [Encripci√≥n asim√©trica](#Encripci√≥n-asim√©trica)
      * 6.4.2.1 [Llaves SSH en Linux](#llaves-ssh-en-linux)
      * 6.4.2.2 [Llaves para HTTP](#llaves-para-http)
    * 6.4.3 [Combinaci√≥n de encripci√≥n sim√©trica y asim√©trica](#Combinaci√≥n-de-encripci√≥n-sim√©trica-y-asim√©trica)
    * 6.4.4 [Uso de certificados - Public Key Infrastructure (PKI)](#Uso-de-certificados---Public-Key-Infrastructure-(PKI))
      * 6.4.4.1 [Autoridad certificante (Certification Authority)](#Autoridad-certificante-(Certification-Authority))
    * 6.4.5 [Uso en Kubernetes](#Uso-en-Kubernetes.1)
      * 6.4.5.1 [Comandos TLS para Linux](#comandos-tls-para-linux)
        * 6.4.5.1.1 [Generar certificado autofirmado para CA](#generar-certificado-autofirmado-para-ca)
        * 6.4.5.1.2 [Generar certificado para usuarios (Admins)](#Generar-certificado-para-usuarios-(Admins))
        * 6.4.5.1.3 [Generar certificado para componentes del sistema](#generar-certificado-para-componentes-del-sistema)
        * 6.4.5.1.4 [Ver detalles de un certificado](#ver-detalles-de-un-certificado)
        * 6.4.5.1.5 [Ver detalles de un CertificateSigningRequest](#ver-detalles-de-un-certificatesigningrequest)
        * 6.4.5.1.6 [Llamado API con certificados](#llamado-api-con-certificados)
    * 6.4.6 [Certificado de Kube-APIServer](#certificado-de-kube-apiserver)
      * * 6.4.6.1.1 [Certificados de Nodos o Kubeletes](#certificados-de-nodos-o-kubeletes)
      * 6.4.6.2 [Estado de salud TLS de un cl√∫ster](#Estado-de-salud-TLS-de-un-cl√∫ster)
    * 6.4.7 [Uso en ETCD](#uso-en-etcd)
    * 6.4.8 [Certificates API](#certificates-api)
      * 6.4.8.1 [Crear un usuario nuevo](#crear-un-usuario-nuevo)
      * 6.4.8.2 [Comandos de Certificates API](#comandos-de-certificates-api)
    * 6.4.9 [Configuraci√≥n de Kubectl - Kubeconfig](#Configuraci√≥n-de-Kubectl---Kubeconfig)
      * 6.4.9.1 [Ubicaci√≥n del archivo](#Ubicaci√≥n-del-archivo)
      * 6.4.9.2 [Contenido del archivo](#contenido-del-archivo)
  * 6.5 [Encryption at rest](#encryption-at-rest)
  * 6.6 [API Groups](#api-groups)
    * 6.6.1 [Core](#core)
    * 6.6.2 [Named](#named)
    * 6.6.3 [Otras consultas API](#otras-consultas-api)
    * 6.6.4 [Authenticaci√≥n API con Kubectl Proxy](#Authenticaci√≥n-API-con-Kubectl-Proxy)
  * 6.7 [Image Security](#image-security)
    * 6.7.1 [Repositorio Privado](#repositorio-privado)
  * 6.8 [Security Contexts](#security-contexts)
    * 6.8.1 [Capabilities](#capabilities)
  * 6.9 [Network Policy](#network-policy)
    * 6.9.1 [Habilitar todo el tr√°fico](#Habilitar-todo-el-tr√°fico)
    * 6.9.2 [Habilitar todo el tr√°fico para un pod](#Habilitar-todo-el-tr√°fico-para-un-pod)
    * 6.9.3 [Habilitar uso de DNS interno](#habilitar-uso-de-dns-interno)
    * 6.9.4 [Denegar todo el tr√°fico](#Denegar-todo-el-tr√°fico)
    * 6.9.5 [Aclaraci√≥n importante sobre los Arrays y los Diccionarios en una NetPol](#Aclaraci√≥n-importante-sobre-los-Arrays-y-los-Diccionarios-en-una-NetPol)
      * 6.9.5.1 [Comandos de Netwrok Policies](#comandos-de-netwrok-policies)
* 7 [![:file_cabinet:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f5c4.png) Almacenamiento](#üóÑ-Almacenamiento)
  * 7.1 [Storage en Docker](#storage-en-docker)
  * 7.2 [Storage Drivers](#storage-drivers)
  * 7.3 [Volume Drivers](#volume-drivers)
  * 7.4 [Container Runtime Interface (CRI)](#Container-Runtime-Interface-(CRI).1)
  * 7.5 [Container Storage Interface (CSI)](#Container-Storage-Interface-(CSI))
  * 7.6 [Volumes (Vol√∫menes)](#Volumes-(Vol√∫menes))
    * 7.6.1 [Persistent Volume](#[hardBreak]Persistent-Volume)
    * 7.6.2 [Persistent Volume Claim](#persistent-volume-claim)
    * 7.6.3 [Storage Classes](#storage-classes)
* 8 [![:globe_with_meridians:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f310.png) Networking](#üåê-Networking)
  * 8.1 [Comandos de Networking](#comandos-de-networking)
    * 8.1.1 [En Linux](#en-linux)
    * 8.1.2 [Trabajar con Namespaces en Linux](#trabajar-con-namespaces-en-linux)
    * 8.1.3 [En Docker](#en-docker)
  * 8.2 [Networking en Docker](#networking-en-docker)
  * 8.3 [Container Networking Interface (CNI)](#Container-Networking-Interface-(CNI))
    * 8.3.1 [Obligaciones del est√°ndar](#Obligaciones-del-est√°ndar)
  * 8.4 [Conexi√≥n de containers a la red](#Conexi√≥n-de-containers-a-la-red)
    * 8.4.1 [Forma general](#forma-general)
    * 8.4.2 [Est√°ndar definido por CNI](#Est√°ndar-definido-por-CNI)
  * 8.5 [Red en Kubernetes](#red-en-kubernetes)
    * 8.5.1 [Node Layer](#node-layer)
    * 8.5.2 [Pod Layer](#pod-layer)
      * 8.5.2.1 [Requerimientos de modelo de red](#requerimientos-de-modelo-de-red)
      * 8.5.2.2 [IP Address Assignment Management (IPAM)](#IP-Address-Assignment-Management-(IPAM))
    * 8.5.3 [Uso de CNI en K8s](#uso-de-cni-en-k8s)
    * 8.5.4 [Weave Net CNI (Network Plugin)](#Weave-Net-CNI-(Network-Plugin))
  * 8.6 [Service Networking](#service-networking)
  * 8.7 [DNS en Kubernetes](#dns-en-kubernetes)
    * 8.7.1 [Kube-DNS (default)](#Kube-DNS-(default))
    * 8.7.2 [DNS para services](#dns-para-services)
    * 8.7.3 [DNS para pods](#dns-para-pods)
    * 8.7.4 [Core-DNS](#core-dns)
  * 8.8 [Ingress](#[hardBreak]Ingress)
    * 8.8.1 [Ingress controllers](#ingress-controllers)
      * 8.8.1.1 [Annotations](#annotations)
    * 8.8.2 [Ingress Route](#ingress-route)
    * 8.8.3 [Ingress class](#ingress-class)
    * 8.8.4 [Caso de Service tipo Load Balancer](#caso-de-service-tipo-load-balancer)
    * 8.8.5 [Comandos Ingress](#comandos-ingress)
* 9 [![:diamond_shape_with_a_dot_inside:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4a0.png) Dise√±o de un cl√∫ster Kubernetes](#[hardBreak][hardBreak]üí†-Dise√±o-de-un-cl√∫ster-Kubernetes)
  * 9.1 [Alta disponibilidad (High Availability)](#Alta-disponibilidad-(High-Availability))
    * 9.1.1 [HA en componentes Stateful de Kubernetes](#ha-en-componentes-stateful-de-kubernetes)
    * 9.1.2 [ETCD en HA](#etcd-en-ha)
  * 9.2 [Opciones de despliegue](#opciones-de-despliegue)
    * 9.2.1 [Creaci√≥n manual de un cl√∫ster (Hard Way)](#Creaci√≥n-manual-de-un-cl√∫ster-(Hard-Way))
    * 9.2.2 [TLS Bootstrapping](#tls-bootstrapping)
    * 9.2.3 [Creaci√≥n de un cl√∫ster con Kubeadm](#Creaci√≥n-de-un-cl√∫ster-con-Kubeadm)
* 10 [![:question:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/2753.png) Troubleshooting](#‚ùì-Troubleshooting)
  * 10.1 [Application](#application)
  * 10.2 [Control Plane](#control-plane)
  * 10.3 [Worker Node](#worker-node)
  * 10.4 [Networking](#networking)
* 11 [![:scroll:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4dc.png) Comandos (Kubectl)](#üìú-Comandos-(Kubectl))
  * 11.1 [Banderas/flags o par√°metros](#Banderas/flags-o-par√°metros)
    * 11.1.1 [Correr en modo simulaci√≥n](#Correr-en-modo-simulaci√≥n)
    * 11.1.2 [Cambiar formato de salida](#cambiar-formato-de-salida)
    * 11.1.3 [Selector por Labels](#selector-por-labels)
  * 11.2 [Generales](#Generales.1)
    * 11.2.1 [Obtener info. del cluster](#Obtener-info.-del-cluster)
    * 11.2.2 [Ver configuraci√≥n actual](#Ver-configuraci√≥n-actual)
    * 11.2.3 [Identificar perfil de configuraci√≥n en uso](#Identificar-perfil-de-configuraci√≥n-en-uso)
    * 11.2.4 [Ver objetos](#ver-objetos)
    * 11.2.5 [Ver eventos](#ver-eventos)
    * 11.2.6 [Desplegar con un manifiesto YAML](#desplegar-con-un-manifiesto-yaml)
    * 11.2.7 [Editar un recurso desplegado en memoria](#editar-un-recurso-desplegado-en-memoria)
  * 11.3 [Cargas de trabajo](#cargas-de-trabajo)
    * 11.3.1 [Crear pod](#crear-pod)
    * 11.3.2 [Exponer un pod](#exponer-un-pod)
    * 11.3.3 [Crear deployment](#crear-deployment)
    * 11.3.4 [Conectarse a Bash de un POD](#conectarse-a-bash-de-un-pod)
    * 11.3.5 [Crear servicio (exponer pods)](#Crear-servicio-(exponer-pods))
    * 11.3.6 [Poner un label a un pod](#poner-un-label-a-un-pod)
    * 11.3.7 [Escalar un deployment](#escalar-un-deployment)
    * 11.3.8 [Actualizar un despliegue](#actualizar-un-despliegue)
    * 11.3.9 [Hacer un rollback de despliegue](#hacer-un-rollback-de-despliegue)
    * 11.3.10 [Cambiar imagen de despliegue](#cambiar-imagen-de-despliegue)
    * 11.3.11 [Forwardear puerto local a Kubernetes](#forwardear-puerto-local-a-kubernetes)
    * 11.3.12 [Reiniciar un deployment sin re-deployar](#reiniciar-un-deployment-sin-re-deployar)
    * 11.3.13 [Desplegar con un manifiesto YAML](#Desplegar-con-un-manifiesto-YAML.1)
    * 11.3.14 [Invocar un servicio por FQDN](#invocar-un-servicio-por-fqdn)
    * 11.3.15 [Ver logs de un pod](#ver-logs-de-un-pod)
  * 11.4 [Otros](#otros)
  * 11.5 [Obtener info de recursos API](#obtener-info-de-recursos-api)
    * 11.5.1 [Debugear DNS](#debugear-dns)
    * 11.5.2 [Crear Config-Map](#crear-config-map)
    * 11.5.3 [Crear secreto](#crear-secreto)
    * 11.5.4 [Esperar una condicion](#esperar-una-condicion)

![:info:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/atlassian/info_64.png) Conceptos
===========================================================================================================

Generales
---------

_Names_ o Nombres: Identificador a nivel API para un objeto de Kubernetes, que consta de hasta 253 caracteres alfanum√©ricos, con guiones medios/bajos y/o puntos.

_UID_: Identificador √∫nico para un objeto de Kubernetes, que el mismo sistema genera de forma autom√°tica y aleatoria.

### Cl√∫ster

Grupo coordinado de _computadoras_ que trabajan como una sola unidad. Los _miembros_ (o _workers_) se llaman nodos. Luego -en el caso de _Kubernetes_\- el _Control Plane_ administra y orquesta esas computadoras, adem√°s de exponer la _API de Kubernetes_ que utiliza para conectarse con los nodos y para que el usuario interactue con el cluster en s√≠.

Open image-20210528-235521.png

![image-20210528-235521.png](blob:https://juannfox.atlassian.net/173192e3-3529-40d8-8d24-2bf9049c0d79#media-blob-url=true&id=d5388231-5f5a-4b8f-8f30-da26b74cb0b4&collection=contentId-2260993&contextId=2260993&width=476&height=385&alt=)

Un cluster productivo debe tener al menos tres nodos (n√∫meros impares con un m√≠nimo de 3). Esto se debe a motivos de alta disponibilidad.

### Recursos

Objetos de _Kubernetes_ con cierta funcionalidad y una estructura personalizada, definida por _K8s_ o incluso por algun tercero (_CustomResourceDefinition_).

Pueden ser _namespaced_ o _clusterscoped_ (asociables a un _Namespace_, o universales al cluster, seg√∫n corresponda).

### Enfoque Imperativo

Enfoque donde los comandos utilizados son de instrucciones expl√≠citas y deben proveerse manualmente los recursos necesarios para lograr un fin.
Los comandos imperativos de kubernetes son: `kubectl create/run/expose/update`, etc.

### Enfoque Declarativo

A diferencia del enfoque Imperativo, en este enfoque particular todas las tareas las realiza el software, a quien solo debemos solicitarle una configuraci√≥n est√°tica que luego interpreta y dispara todas las acciones necesarias.
Los comandos `kubectl apply -f` y `create -f` (provistos de un manifiesto) son declarativos.

### Manifiesto

Un _manifiesto_ es un archivo que declara caracter√≠sticas y estado de un objeto/recurso de _Kubernetes_. El lenguaje utilizado para estos manifiestos es _YAML_.

Si bien existen manifiestos para todo tipo de recursos, a gran escala se puede visualizar siempre la estructura b√°sica:

`apiVersion: <Version de la API> kind: <Tipo de recurso u objeto> metadata: #Identificadores de objeto name: <nombre> labels: <etiqueta1>: <valor1> spec: <Configuraciones de objeto>`

Las secciones _Metadata_ y _Spec_ tienen formato de diccionario, con una estructura de jerarqu√≠as (valores padre e hijos).

Las listas/arrays arrancan sus items con ‚Äú-‚Äù. La diferencia con -por ejemplo- los labels de Metadata es que los items tienen propiedades hijas (estos son diccionarios, no listas).

Las listas o arrays se pueden utilizar de las dos formas descritas a continuaci√≥n:
\-Utilizando items ordenados con ‚Äú`-`".
\-Utilizando lista CSV entre corchetes, con sus valores entre comillas: `["1","2","3"]`

#### Estados de un manifiesto

Un manifiesto tiene tres ‚Äústages‚Äù o etapas:

1)**Manifiesto del usuario (YAML):** Utilizado mediante Kubectl Apply por el usuario

2)**Last applied configuration (JSON):** Versi√≥n JSON de la √∫ltima configuraci√≥n aplicada por el usuario, osea el archivo utilizado para crear el objeto. Tambi√©n se almacena sobre el archivo en vivo como una annotation llamada _kubectl.kubernetes.io/last-applied-configuration_.

3)**Live update configuration:** Versi√≥n en memoria del manifiesto (YAML) que tiene propiedades extra que denotan el estado del objeto en vivo (runtime).

Cuando se hace un cambio en el manifiesto local y se aplica, este se compara con el manifiesto en vivo y luego se aplica. Luego, este cambio se traslada al archivo de √∫ltima configuraci√≥n (este √∫ltimo sirve como referencia).

Los manifiestos en runtime tienen campos extra con informaci√≥n de estado, almacenados en memoria.

Como una de las ventajas de trabajar con manifiestos es la posibilidad de tener un respaldo de como c√≥digo en un repositorio de la infraestructura, no es recomendado hacer cambios imperativos ‚Äúen vivo‚Äù sobre los objetos desplegados en Kubernetes, ya que ese cambio queda solo en memoria y no en el manifiesto almacenado como archivo por el usuario.

### Label

Atributo de tipo par _key:_value, que se puede definir en un manifiesto de un recurso. Luego un _selector_ de servicio puede identificar ese valor, por ejemplo (o simplemente servir como etiqueta/tag).

Debe tener m√°ximo 63 carateres y ser un subdominio DNS v√°lido. En caso de incluir un prefijo (no obligatorio), se lo separa con ‚Äú/‚Äù y tiene m√°ximo 253 caracteres.

#### Label Selector

Permite identificar aquellos objetos que contengan cierto Label. Los operadores posibles son **\=** y **!=**, aunque tambi√©n soporta listas con los operadores **in**, **notin** y **exists**:

`role = webserver role != webserver role in (webserver,httpserver)`

##### Queries con Label Selector

`kubectl get pods -l environment=production,tier=frontend kubectl get pods -l 'environment in (production,qa),tier notin (frontend)'`

### Annotation

Metadato de tipo par _key:value_ que sirve √∫nicamente para cargar de informaci√≥n distintiva a un objeto de Kubernetes.

Algunas integraciones no nativas de Kubernetes aprovechan estos campos para inyectar configuraciones extra (ejemplo Ingress Controllers).

Lenguajes
------------

Es facil convertir valores entre _YAML_ y _JSON_, o viceversa.

### YALM

Es un lenguaje que representa propiedades, soportando pares _clave: valor_ (separados por un espacio), arrays/vectores (o listas) y maps o diccionarios. La indentaci√≥n es obligatoria y debe ser consistente, representada con espacios.

Los comentarios se hacen con el caracter ‚Äú#‚Äù.

#### Pares clave-valor

`clave1: valor1 clave2: valor 2`

#### Array

`clave1: - subclave1: subvalor 2 - subclave2: subvalor1`

#### Map o Diccionario

`clave1: subclave1: subvalor1 subclave2: subvalor2`

No tiene orden de subitems.

Es posible combianr _Arrays_ y _Diccionarios_.

### JSON (JavaScript Object Notation)

Lenguaje estructurado de Java que representa datos, soportando pares _clave: valor_ (separados por un espacio), _arrays_/_vectores_ (o _listas_) y _maps_ o _diccionarios_. La indentaci√≥n es obligatoria y debe ser consistente, representada con espacios.

_JSON_ es un objeto v√°lido de _Javascript_, por lo que puede utilizarse en dicho lenguaje para almacenar informaci√≥n.

* Tipos de datos: String, n√∫mero (no difiere entre Int o Float), Boolean (True o False), Nulo (null) y objeto ({‚Äúclave‚Äù,‚Äùvalor‚Äù}.

* Comentarios: No existen.

Para convertir un _JSON_ en formato _String_ a un _JSON_ v√°lido en _Javascript_ se utiliza _JSON.parse()_. El procedimiento inverso es _JSON.stringify()_.

#### Queries (JSONPath)

Similar a trabajar con objetos, los campos de un item se pueden obtener con queries usando puntos, indicando la propiedad del item:

`item1.propiedad1.subpropiedad1`

El _elemento de nivel superior_ (o top level) se llama _root_ y se utiliza con el caracter ‚Äú**$**‚Äù:

`$.item1.propiedad1.subpropiedad1`

Los resultados vienen dentro de un array (\[\]).

Para queries sobre listas, se utiliza la posici√≥n (similar a C++), iniciando en 0:

`$[0].item1.propiedad1.subpropiedad1`

Luego para seleccionar m√∫ltples elementos utilizar el caracter ‚Äú,‚Äù e indicar inicio,fin:

`$[0,4]`

Para una selecci√≥n espec√≠fica, al igual que en _Pyhton_, utilizar el caracter ‚Äú:‚Äù con formato \[inicio:final:intervalo\]:

`$.[0:4:2]`

Para el √∫ltimo item de la lista, se utiliza el √≠ndice inverso, que inicia con ‚Äú-1‚Äù en el √∫ltimo elemento de la lista. Por lo que puede utilizarse:

`$[-1]`

O bien, en algunos casos:

`$[-1:0]`

Se pueden utilizar _criterios_ v√°lidos para filtrar el resultado, ubicando la _condici√≥n_ dentro de los corchetes. El signo ‚Äú?‚Äù abre el condicional y luego la condici√≥n se chequea dentro de los par√©ntesis como operaci√≥n, donde ‚Äú@‚Äù es el iterador de ‚Äútodos los valores‚Äù (para chequeo c√≠clico). El operador puede ser: >, <, ==, !=, in o nin.

Ejemplo de uso:

`$[?(@>40)]`

Ejemplo avanzado:

`$.item1.subitem2[?(@.propiedadsubitem1 == "valorDeseado" )].(@.propiedadsubitem2`)

Tambi√©n se pueden combinar queries con comas:

`[?(query1),?(query2)]`

El caracter ‚Äú\*‚Äù actua como _wildcard_ en los queries de _JSONPath_.

`$.*.subpropiedad1`

o bien para arrays:

`$.[*].subpropiedad1`

#### Array

Los Arrays se definen entre corchetes o brackets (\[\]) y sus subvalores entre llaves o curly braces ({}), mientras que cada item del array se separa por ‚Äú,‚Äù.

`{ "clave1": { "subclave1": "subvalor2", "subclave2": [ { "clavearray1": "valorarray2", "clavearray1": "valorarray2", }, { "clavearray1": "valorarray2", "clavearray1": "valorarray2", } ] } }`

#### Map o Diccionario

Los items se definen entre llaves o curly braces ({}):

`{ "clave1": { "subclave1": "subvalor2", "subclave2": "subvalor2" } }`

#### JSONPath en K8s

[https://kubernetes.io/docs/reference/kubectl/jsonpath/](https://kubernetes.io/docs/reference/kubectl/jsonpath/)

El root ‚Äú$‚Äù lo agrega Kubectl por defecto, por lo que puede omitirse.

Utilizar {‚Äú\\n‚Äù} crea nuevas l√≠neas.

Usar comandos _kubectl_ con par√°metro:

`--jsonpath <query>`

O bien

`-o=jsonpath='<query>'`

Tambi√©n se pueden combinar queries, utilizando ‚Äú{}{}‚Äù para separarlas.

El operador ‚Äúrange‚Äù permite hacer un ciclo for sobre los resultados:

`{range .item[*]}{metadata.name}{end}`

##### Operaciones complejas - Columnas y orden

Tambi√©n se puede combinar con columnas personalizadas:

`kubectl get nodes -o=custom-columns=<nombreColumna1>:<query1><nombreColumna2>:<query2>`

Luego el par√°metro `---sort-by` permite ordenar segun un _JSONPAth_.

##### Queries condicionales

Los queries con condicionales tienen un formato distinto. Ejemplo para obtener la IP de un nodo:

`kubectl get node <nodo> -o=jsonpath='{.status.addresses[?(.type == "InternalIP")].address}'`

Es importante entender que los comandos _GET_ de _K8s_ que se env√≠an sin un nombre particular devuelven la lista de todos los elementos de ese tipo en el _Namespace_, dentro de un _Array_ llamado _Items_ (parte del objeto _List_ de _Kubernetes_). Esto afecta las queries, ya que cada elemento es un item del array de la lista.

Para obtener las IPs de todos los nodos, en cambio, no se env√≠a el nombre del nodo y adem√°s se hace el query sobre el parent _items_, con el √≠ndice \* (wildcard):

`kubectl get node -o=jsonpath='{.items[*].status.addresses[?(.type == "InternalIP")].address}'`

Master components
-----------------

Los componentes maestros componen los clusters de _Kubernetes_ y pueden estar distribuidos en varios _nodos_, o sobre un √∫nico _nodo_.

Cada componente se conecta al cl√∫ster con perfiles de configuraci√≥n guardados en archivos _Kubeconfig_ (de _Kubectl_) y asegura la conexi√≥n con _encriptaci√≥n TLS_ punta a punta.

Las opciones de instalaci√≥n incluyen:

1. Instalar cada componente como un _servicio_ de _Linux_ y proveerles la configuraci√≥n _Kubeconfig_ y _TLS_ necesaria.

2. Aplicar _Kubernetes_ a _Kubernetes_, instalando los componentes como _Static Pods_ dentro de los nodos, gracias al _Kubelet_ \-que ya debe estar instalado como servicio- (incluyendo la configuraci√≥n _Kubeconfig_ y _TLS_ necesaria).

Como la opci√≥n 2 es factible, entonces es posible armar un cl√∫ster de _K8s_ con solo el _Kubelet_ instalado. Este m√©todo es el utilizado por _Kubeadm_.

### Kube-APIServer

Expone la _Rest API_ de _Kubernetes_ (_control plane_) y es el componente principal. Tambi√©n se conoce como _Master_.

### ETCD

Es un tipo de base de datos distribuida de pares _clave:valor_ (key:value) que se caracteriza por ser confiable, r√°pida y segura. Se denomina como un _Distributed Configuration System_ (_DCS_) por el tipo de uso que recibe.

El puerto por defecto para el servicio es el 2379 (TCP).

Tiene dos APIs distintas, v2 (standard) y v3 (que se setea con la enviroment variable ETCDCTL\_API=3).

#### Funcionamiento

A diferencia de una base de datos _relacional_ (o _tabular_), donde una serie de ‚Äúllaves‚Äù (o claves) son √∫nicas y ordenan una serie de entradas que son compatibles con ese formato, en una base de datos _ETCD_ cada entrada est√° compuesta por un conjunto de llaves y valores (en formato llave:valor) y compone por si misma una estructura, sin necesidad de un esqueleto de ‚Äútabla‚Äù √∫nica (adem√°s de que puede o no variar en ‚Äúlargo‚Äù de otros objetos similares). Una estructura similar a la utilizada por este tipo de DB es el diccionario de _Python_ o el Hashtable de _Powershell_.

#### Alta disponibilidad

Idealmente el servicio deber√≠a estar clusterizado (tipo HA, High Abailability) con varias instancias y un _ETCD API Server_ (o master), segurizado mediante el uso de certificados TLS/SSL.

#### Uso cl√°sico

En _Linux_ se instala como servicio y tiene una interfaz de l√≠nea de comando llamada _etcdctl_, similar a otras herramientas de dicho SO.

#### Uso en Kubernetes

En Kubernetes se implementa adoptando _HA_, usualmente desplegado como un _Static Pod_ sobre el nodo _ControlPlane_ y se utiliza el _CA_ (_Certification Authority_) de _Kube-APIServer_ o uno propio de _ETCD_. Cumple el rol de guardar informaci√≥n y estado de los diferentes componentes (nodos, pods, configs, secrets, accounts, roles, bindings, etc).

N√≥tese que es un rol crucial, ya que guarda el estado en vivo de los manifiestos de recursos desplegados en el cl√∫ster.

### Kube Controller Manager

Es un agente encargado de revisar el estado -y remediar posibles problemas- de los diferentes componentes de Kubernetes, lo cual hace mediante el uso del canal que provee el _Kube-APIServer_. Est√° compuesto por una serie de controladores, que se enfocan en diferentes tareas de forma permanente.

#### Ejemplos de controllers

El Node Controller, particularmente, utiliza probes de salud con cierta parametrizaci√≥n (frecuencia, per√≠odo de gracia y plazo de desalojo de pods). Si un nodo est√° marcado como _unreachable_ (o _not ready_) por al menos el per√≠do de gracia y el plazo de desalojo, le quita la asignaci√≥n de sus pods y los re-asigna a otros nodos.

En cambio, el Replication Controller apunta a controlar que los _Replicasets_ cumplan la cantidad de pods que declaran, creando nuevos -y elminando pods con problemas- cuando sea necesario.

#### Cloud controller manager

Controlador del proveedor de nube que se integra con Kubernetes (si es soportado) y reemplaza ciertas funcionalidades del controlador _Kube_ nativo, para cederlas a la plataforma de hosting (roles, vol√∫menes, etc).

### Kube-Scheduler

Es un _planificador_ que se encarga de decidir -en base a ciertos criterios- en qu√© _nodos_ ubicar los _pods_ (las cargas de trabajo), lo cual se actualiza adem√°s de forma din√°mica.

Notese que este componente no se encarga de crear el recurso, sino de decidir cuando y donde ubicarlo (en qu√© nodo). Luego la tarea de crear el recurso la realiza el _Kubelet_ del nodo correspondiente.

#### Criterios de decisi√≥n

Para decidir, primero filtra los nodos no compatibles (en base a la cantidad de recursos necesarios) y luego establece un ranking de los nodos restantes (en base a _recursos libres_), definiendo la/s mejor/es opcion/es. El _scheduler_ adem√°s interpreta reglas de _NodeAffinity/Anti-Affinity, NodeSelectors_ y _Taints/Tolerations_, tom√°ndolas en cuenta al decidir en qu√© nodo ubicar una carga de trabajo.

Si no existiera el _scheduler_, ser√≠a posible hacer scheduling manual, agregando la propiedad _nodeName_ en la configuraci√≥n _spec_ del pod.

#### Custom-Scheduler (_Scheduler personalizado)_

Se pueden tener varios _kube-scheduler_ (en casos de multi-master en un cluster), o incluso desplegar otros schedulers personalizados, idealmente como pod est√°ticos en un nodo. En este caso, entra en juego el proceso de _leader-election_ para definir cual de los schedulers de la misma clase es el lider y el parametro _\--lock-object-name_ para diferenciar los schedulers custom de los originales. El campo _scheudlerName_ en un pod le indica qu√© scheduler debe manejarlo.

Manifiesto de ejemplo del despliegue de un _Scheduler_ personalizado como pod:

Open

### Container Runtime Interface (CRI)

Es un motor compuesto por un conjunto de est√°ndares, requerimientos, APIs _gRCP_ y librerias, que permite que los _container runtimes_ se integren con el _kubelet_.

El _kubelet_ no se debe comunicar directamente con un container runtime, ya que se comunica con ellos mediante el _CRI shim_ (una interfaz API) y por ende amplia la compatibilidad a cualquier runtime que sea _CRI compliant_.

Luego existen dos interfaces gRPC: _ImageService_ y _RuntimeService_, responsables por el manejo de im√°genes y de contenedores, sucesivamente.

Es por esto que no solo las im√°genes de _docker_ son compatibles con _Kubernetes_, sino cualquier imagen de _contenedor_ que cumpla el est√°ndar _CRI_.

Open

Otros componentes
-----------------

### Namespace

Divisi√≥n l√≥gica dentro de un cl√∫ster que permite aislar objetos de kubernetes, proporcionandoles un ambiente con sus propias policies, recursos y red.

Se pueden invocar servicios de otro namespace utilizando el formato _<servicio>.<namespace>.svc.cluster.local_ (corresponde a una entrada _DNS_ autogenerada por el servicio).

Para setear un NS por defecto, utilizar el comando:

`kubectl config set-context $(kubectl config current-context) --namespace=MiNamespace`

Ejemplo de manifiesto:

`apiVersion: v1 kind: Namespace metadata: name: minombre`

#### ResourceQuota

Es posible definir una cuota de recursos a asignar a un _Namespace_, limitando la cantidad disponible dentro del mismo (que se complementa con la disponibilidad de los nodos):

`apiVersion: v1 kind: ResourceQuota metadata: name: minombre-quota namespace: minombre spec: hard: pods: 10 requests.cpu: 4 requests.memory: 5Gi`

### Container

Un contenedor es una abstracci√≥n en la capa de aplicaci√≥n, que est√° compuesto por el c√≥digo y sus dependencias, pero no por el sistema operativo en s√≠ (a diferencia de una _Virtual Machine_); en pocas palabras, no tiene _Kernel_. Luego varios contenedores pueden convivir aislados sobre un mismo host, donde utilizan procesos separados.

Los containers que soporta Kubernetes son de tipo _Open Container Iniciative_ (_OCI_).

#### Imagen

Es una serie de instrucciones utilizadas para armar un contenedor.

#### Comandos y argumentos

Los containers tienen un ciclo de vida y no fueron concebidos para alojar un _SO_ (a diferencia de una _VM_). Por ende, muchas veces es necesario enviar un comando especial al container para que se mantenga ‚Äúvivo‚Äù o activo.

Los comandos se pueden enviar como _string_ (1) o como _array_ (2):

1)`‚Äù<Comando> <Parametro1> <Parametro2>"`

2)`["Comando","Parametro1","Parametro2"]`

Los comandos pueden ejecutarse como parte de la imagen del contenedor, as√≠ como tambien especificados en el manifiesto del pod (sobreescribe el de la imagen).

En una imagen de docker, la propiedad _EntryPoint_ indica el binario a ejecutarse por defecto, al cual se le agregan los par√°metros enviados al ejecutar la imagen (ej: Con ‚Äúsleep‚Äù como _EntryPoint_, ejecutar la imagen con el par√°metro ‚Äú5‚Äù equivale a ‚Äúsleep 5‚Äù). Al definir un comando (con _CMD_) por defecto, actua como el valor de par√°metro por defecto para el _EntryPoint_:

`FROM python:3.6-alpine RUN pip install flask COPY . /opt/ EXPOSE 8080 WORKDIR /opt ENTRYPOINT ["python", "app.py"] CMD ["--color", "red"]`

En el ejemplo al iniciar el container se ejecuta ‚Äúpython appy.py --color red‚Äù, a no ser que se env√≠e un argumento, en cuyo caso se ejecuta ‚Äúpython app.py <argumento>‚Äù.

En la definici√≥n del pod, el campo _command_ se corresponde con el _EntryPoint_ del container y el campo _args_ con el de _CMD_ del container (a ambos los sobreescribe):

**POD**

**Imagen**

**POD**

**Imagen**

_command_

_EntryPoint_

_args_

_CMD_

Ejemplo:

`apiVersion: v1 kind: Pod metadata: name: webapp-green labels: name: webapp-green spec: containers: - name: simple-webapp image: kodekloud/webapp-color args: ["--color","green"]`

En el ejemplo, se ejecuta ‚Äúpython appy.py --color green‚Äù (en vez de ‚Äúred‚Äù como dice en la imagen), porque los argumentos (Args) del YAML sobreescriben el comando (CMD) del container. Si en cambio se enviara un ‚Äúcommand‚Äù en el YAML, este sobreescribir√≠a el binario anunciado en el EntryPoint del archivo Docker.

### Pod

Unidad at√≥mica de Kubernetes, que es pr√°cticamente una m√°quina virtual liviana, donde corren uno o m√°s _containers_ como procesos aislados, pero compartiendo recursos (red, almacenamiento, CPU y memoria).

Este recurso es el que aloja la carga de trabajo en _Kubernetes_ y es utilizado de una manera u otra por las diferentes abstracciones que la plataforma trae (_Deployments_, _Statefulsets_, _Daemonsets_ y _Jobs_).

Est√° pensado para ser ef√≠mero y logra eso gracias a que se basa en un _template_ declarativo (dentro de un manifiesto _YAML_) que le indica los par√°metros de creaci√≥n, asegurando que todas las instancias de recurso que se creen con el mismo sean id√©nticas (excepto por el _ID_ de recurso y la _IP_). Un _Pod_ no se reinicia, se elimina y se crea uno nuevo. Es por esto que no debe adoptarse afinidad con el mismo, sino considerarlo una ‚Äúcopia‚Äù o ‚Äúclon‚Äù de una plantilla de recurso.

Como el _Pod_ no tiene persistencia, es de esperar que al reiniciarse -o eliminarse- se pierdan los datos escritos en memoria (y no solo _RAM_), por lo que el _Storage_ suele proveerse de forma externa. Adem√°s, la _IP_ cambia con cada reinicio (para lo cual existe un rango _DHCP_ de _IPs_ para _pods_) y no tiene nombres _DNS_ (si bien pueden agregarse, aunque no recomendados).

La forma de identificar la _IP_ de un _Pod_ -que cambia a menudo- sin _DNS_ es mediante el uso de _Labels_ (etiquetas sobre el mismo) y un _Selector_. Esto se explica posteriormente en la secci√≥n de _Servicios_.

#### MultiContainer

En ocasiones puede ser util correr m√∫ltples contenedores en un pod, ya que entre ellos comparten red y almacenamiento. Algunas aplicaciones requieren de un companion o una funcionalidad externa, que puede acoplarse de esa forma.

El YAML de definici√≥n del container es un array, por lo que -para a√±adir otro container- basta con agrega otro item (-).

Open

**\-Sidecar:** Cumple alguna funcion que se complementa con la aplicaci√≥n y es de utilidad que compartan el host (pod).

**\-Adapter:** Para convertir el tipo de tr√°fico o los datos enviados a/desde la aplicaci√≥n.

**\-Ambassador:** Actua como una especie de Proxy para la aplicaci√≥n.

#### InitContainers

Todos los containers en un pod _MultiContainer_ deben mantenerse ‚Äúvivos‚Äù para que el pod lo haga tambi√©n; si uno falla o se apaga, el pod falla. Por eso los _InitCointaners_ son de utilidad, ya que corren una tarea y luego pueden dejar de funcionar, sin afectar al pod.

N√≥tese que los containers se crean solo cuando los _InitContainers_ corren su ciclo completo.

En caso de configurar varios containers de este tipo, corren en orden secuencial; si alguno de los _IC_ falla, el pod se reinicia.

Ejemplo de manifiesto:

`spec: containers: ¬† - name: myapp-container ¬†¬†¬† image: busybox:1.28 ¬†¬†¬† command: ['sh', '-c', 'echo The app is running! && sleep 3600'] ¬† initContainers: ¬† - name: init-myservice ¬†¬†¬† image: busybox ¬†¬†¬† command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']`

#### Afinidad

##### Taints and tolerations

Marca (del ingl√©s olor o mancha) que se aplica sobre un nodo para repeler pods que no tienen tolerancia al mismo. Luego se especifica _tolerance_ en los pods para que puedan ‚Äúsoportar‚Äù ciertos nodos que tienen _taints_ ‚Äúaceptables‚Äù.

[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)

Cuando el scheduler desea asignar un pod a un nodo, pero encuentran taint al que no es tolerante, lo pasa a otro nodo y descarta ese.

Para marcar un nodo con un _taint_, utilizar el comando:

`kubectl taint nodes <nodeName> <key>=<value>:<taint-effect>`

Tipos de _Taint-Effect_:

**\-No Schedule:** No se asignan nuevos pods si no son tolerantes.

**\-Prefer No Schedule:** A no ser que sea el ultimo nodo disponible, no se aceptan asignaciones de pods no tolerantes.

**\-No execute:** No se asignan nuevos pods intolerantes Y no se ejecutan los que ya existen (_evict_).

Para agregar tolerancias a un Pod, agregar el siguiente template a spec:

Ejemplo de manifiesto:

`tolerations: -key : app operator: "Equal" value: "blue" effect: NoSchedule`

N√≥tese que la toleraci√≥n describe el taint en s√≠.

Los taints/tolerations solo indican en que nodo no se puede asignar un pod, no en cual se va a terminar asignando; para eso existe podAffinity. Un pod solo puede asignarse a los nodos sobre los cuales no tenga ‚Äúalergia‚Äù a los taints.

Se puede omitir la llave _Value_ cuando est√° presente el operador _Exist:_

`tolerations: - key: "example-key" operator: "Exists" effect: "NoSchedule`

* Al utilizar una _Key_ vac√≠a -con valor {}- y el operator _Exists_, implica que tolera todo tipo de _Taints_.

* Utilizar el campo _effect_ vac√≠o -con valor {}- intenta utilizar el efecto de la primer _Key_.

##### Node Selectors

Se pueden tagear los nodos con _labels_ utilizando el comando:

`kubectl label node <nodo> label:value`

De esta forma, se pueden ‚Äúclasificar‚Äù los nodos para que un pod pueda valrse de dicha clasificaci√≥n y preferir cierto tipo de nodo.

Sobre el pod se aplica un selector de labels de nodos (sobre spec):

`nodeSelector: label: value`

Si un pod se est√° ejecutando cuando se modifica un _label_ sobre el nodo y este no coincide con el esperado por su _selector_, se desaloja del nodo.

La gran limitaci√≥n que ofrece esta funci√≥n es que no pueden buscarse nodos que cumplan dos diferentes labels a la par, sino solo aquellos que cumplan las dos labels (ejemplo: Size medium, size large, not size small). Para ello, usar Affinity.

Otra forma algo primitiva de asignar un pod a un nodo es utilizar la propiedad _nodeName_ y el nombre de un nodo en su secci√≥n _spec_.

#####

Node Affinity and Anti-Affinity

Permite hacer selecciones complejas de nodos, para aquellos pods que requieren cierto tipo de nodo espec√≠fico y que la funci√≥n de Node Selector no alcanza a identificar (m√∫ltiples requisitos).

Se agrega en la secci√≥n _spec_ del pod:

`affinity: nodeAffinity: requiredDuringSchedulingIgnoreDuringExecution: nodeSelectorTerms: - matchExpressions: -key: operator: value:`

Es posible definir un tipo de Node Affinity m√°s leve, que es _preferredDuringSchedulingIngoreDuringExecution_, para que se cree de todas formas el pod si no se cumple la regla en ning√∫n nodo.

Se pueden utilizar los operadores _In/NotIn_ y _Exists/DoesNotExists_ (no llevan el campo _value_).

#####

Combinaci√≥n de Node Affinity y Taints/Tolerations

Como ambos tienen de por s√≠ algunos casos de ‚Äúescape‚Äù o excepciones donde no alcanzan a asegurar que los pods correctos -y solo esos- se asignaran a ciertos nodos, pueden combinarse para asegurar al 100% la asignaci√≥n.

Despues de todo, Tains/Tolerations definen los nodos que ‚Äúno tolera‚Äù un pod y Node Affinity aquellos que prefiere un Pod, cubriendo el 100% de los casos.

#### Resources and Requests

##### Equivalencias de unidades

**\-CPU:** 1 cpu == 1000m (milicores) = 1 vcpu +-

**\-Memoria:**

* Escala ‚ÄúClasica‚Äù: 1 Kb (Kilobyte) = 1024 bytes

* Escala ‚ÄúKilo‚Äù: 1 Ki (Kibibyte) = 1000 bytes

##### Acciones de l√≠mites

**\-CPU:** _Throttle_ al llegar al l√≠mite.

**\-Memoria:** Terminar el pod.

##### Valores por defecto

Estos solo existen si se crea un LimitRange sobre el namespace:

`apiVersion: v1 kind: LimitRange metadata: name: resources-limit-range spec: limits: - default: memory: 512Mi cpu: 1 defaultRequest: memory: 256Mi cpu: 0.5 type: Container`

#### Variables de entorno

Se puede enviar informaci√≥n montada como _variables de entorno_ hacia un container de un pod.

Las _Enviroment Variables_ se cargan al inicio del _Pod_, por lo que es necesario reiniciarlo (eliminarlo y crear uno nuevo) para que tome un cambio.

Uso normal:

`env: - name: <nombre> value: <valor>`

Uso con un provider (_ConfigMap_ o _Secret_):

`env: - name: <nombre> valueFrom: configMapKeyRef: name: <nombreDeCM> key: <llaveAInsertar>`

Uso con un provider como lista (array):

`envFrom: - configMapRef: name: <nombreDeCM`

Importante: Cambia el t√©rmino _configMapKeyRef_ por _configMapRef_, ya que se monta todo el archivo y no solo una llave dentro del mismo.

### Node

Virtual Machine que actua como worker dentro de un cluster. Luego el _kubelet_ es el agente que administra el nodo y se comunica con el _Control Plane_ del cluster. Adem√°s, debe de contener alguna herramienta para el manejo de contenedores -o continer runtime-, como _containerd_ o _docker_ (ya no soportado).

#### Node-Components

#### Kubelet

Agente instalado sobre los nodos que efectua las tareas que le env√≠a el _Kube-APIServer_, en base a los tiempos del _Scheduler_. El _kubelet_ se encarga de manejar el _Container Runtime Engine_ del nodo para crear containers y pods.

##### Kube-Proxy

Realiza tareas de networking a bajo nivel sobre los nodos, reflejando los servicios localmente y admitiendo forwardeo _TCP/UDP_; obtiene las IPs del cluster mediante _DNS_ o _variables de entorno_.

En el caso de los _services_, les provee una _IPTable_ para redirigir el tr√°fico hacia un pod, cuando corresponda.

#### Static Pods

El Kubelet trabaja a nivel pod, por lo que -sin el apoyo del _Master_ y todos sus controladores- podr√≠a √∫nicamente generar pods; esto lo puede hacer sin necesidad de estar en un cl√∫ster, pero no puede crear otro tipo de recursos de _K8s_.

A los pods creados por un nodo mediante un mainifiesto local (sobre el host) se les llama _Static Pods_ y es una gran herramienta para desplegar componentes del sistema sobre el _ControlPlane_ (aplicar _Kubernetes_ a _Kubernetes_). De esta forma, es posible crear un _Master_ con solo instalar el _Kubelet_ y el resto de los componentes como _Static_ _Pods_ (t√°ctica aplicada por la herramienta _KubeAdm_).

La direcci√≥n _IP_ de un _Static Pod_ es la misma que la del nodo.

Los manifiestos sobre el nodo se guardan en una ubicaci√≥n fija, la cual est√° definida por el par√°metro _\--pod-manifest-path_, o bien el par√°metro _StaticPodPath_ dentro del archivo _\--config_ (_YAML_) con el que se inicia el _Kubelet_.

Para ver ruta en linux, ejecutar:

`#Mostrar procesos linux, filtrar por Kubelet y filtrar por config ps -aux | grep kubelet | grep config #Buscar en la ruta del config el staticPodPath por Regex cat <rutaConfig>/config.yaml | grep -i ^static #Tomar nota de la ruta`

Si se crean Static Pods en un cl√∫ster con Kube-APIServer, este los puede visualizar, pero solo muestra un objeto espejo de ellos, en modo read-only.

Para identificarlos y distinguirlos de un pod normal, notese que se agrega el sufijo ‚ÄúNombreDeNodo‚Äù al nombre del pod.

### Replica Set

Objeto que asegura que se cumple la cantidad de r√©plicas de pod definida est√© activa y saludable. Tiene la potestad de agregar pods seg√∫n necesite, o tambi√©n quitarlos. Utiliza un template para la creaci√≥n de Pods, que b√°sicamente lleva la configuraci√≥n que llevar√≠a un manifiesto del pod (su metadata y specs).

A diferencia de los pr√°cticamente deprecados _Replication Controllers_ (que trabajaban por nombre), estos objetos identifican pods dentro del grupo de trabajo, basados en una selecci√≥n de set . Para ello, el ReplicaSet utiliza un _selector_ que identifica los pods por _labels_ (por lo que puede tambi√©n manejar pods que no se crearon con el template del RS en s√≠); este selector debe estar definido en la secci√≥n metadata.labels del pod.

Atenci√≥n: Utilizar ‚Äúkubectl scale‚Äù no se edita el RS o RC, por lo que no es la forma recomendada de escalar una soluci√≥n.

Ejemplo de manifiesto para un _RS_:

`apiVersion: apps/v1 kind: ReplicaSet metadata: name: nombre labels: <ReplicaSetLabel1> : <ReplicaSetValor1> spec: templates: metadata: <PodLabel1> : <PodValor1> spec: - name: image: replicas: cantidad selector: matchLabels: <PodLabel1> : <PodValor1>`

N√≥tese que los _labels_ del _RS_ no son los mismos que los del pod, adem√°s de que estos √∫ltimos deben estar indicados en el _selector_.

### Deployment

Manifiesto que indica a Kubernetes c√≥mo actualizar y desplegar una aplicaci√≥n contenerizada. Luego el _Deployment Controller_ monitorea continuamente el estado de esa aplicaci√≥n. Es un recurso muy similar a un ReplicaSet, y de hecho utiliza un ReplicaSet al crearse, pero le agrega a este funcionalidades extra como el de actualizar la aplicaci√≥n o hacer un rollout.

Ejemplo de manifiesto:

`apiVersion: apps/v1 kind: Deployment metadata: blabla spec: templates: metadata: <PodLabel1> : <PodValor1> spec: - name: image: replicas: cantidad selector: matchLabels: <PodLabel1> : <PodValor1>`

N√≥tese la similitud respecto del manifiesto de un _ReplicaSet_, adem√°s de que se utiliza el mismo sistema de _selector_ para identificar los pods.

#### Tipos de despliegue

Un cambio en el _Deployment_ implica dispara una acci√≥n _rollout_, donde se actualizan todos los _pods_ para cumplir con el nuevo _template_. Existen diferentes estrategias para lograr esta actualizaci√≥n:

##### Rolling Update

Se crea un nuevo _ReplicaSet_ en paralelo al existente, el cual incorpora r√©plicas de pods con el template nuevo, a medida que decomisa r√©plicas con el template viejo. Para evitar downtime durante este proceso, Kubernetes no quita una instancia de pod vieja hasta no disponer de una instancia nueva respondiendo correctamente.

El campo _strategyType_ del deployment indica el tipo utilizado en esa instancia. Adem√°s, existen campos como _MaxSurge_ que personalizan la agresividad del despliegue.

##### Recreate

Se quitan las versiones viejas y se despliegan las nuevas, todo en simult√°neo. Dem√°s esta decir que esta opci√≥n no es recomendada.

### Service

_Balanceador de carga_ para los pods que actua de forma virtual y corre sobre la memoria del cl√∫ster; forwardea el tr√°fico recibido hacia los pods que cumplen ciertos requisitos de tagging (_label selectors_).

Para poder almacenar las IPs de los pods de destino, utiliza un objeto llamado _Endpoint_ que tiene una serie de records con las IPs de todos los pods que cumplen con el _selector_. En ocasiones especiales estos pueden administrarse manualmente.

#### Tipos de servicio

##### LoadBalancer

Modo compatible con proveedores cloud que hace un balanceo de carga entre los NodePorts de todos los nodos donde el servicio expone trafico.

##### NodePort

Expone el servicio en un puerto del/los nodos. Esta opci√≥n no es viable para un cl√∫ster autoescalable o con nodos gen√©ricos.

##### ClusterIp

Expone una IP de servicio y redirige el tr√°fico al backend.

##### Headless (ac√©falo)

Servicio sin una IP frontal, para aquellos pods que no requieren balanceo. En cambio se arma una lista de records DNS tipo ‚ÄúA‚Äù para los pods que cumplan con el _Selector_ y es gracias a eso que los pods son accesibles por otros servicios mediante consulta DNS. Alternativamente puede no utilizarse un selector y crearse un _Endpoint_ manualmente.

#### Especificaciones

\-Un servicio tiene su propia IP, a la cual se llama clusterIP.

\-Los servicios operan en _capa 3_ (TCP/UDP).

\-Los puertos que puede usar de un nodo son entre 30000 y 32767.

\-Utilizar ‚ÄúclusterIp‚Äù define la IP manualmente (adem√°s del tipo de SVC).

Si no se provee un puerto de servicio, se utiliza el mismo que el _TargetPort_. Si no se provee un NodePort, se asigna uno al azar del rango permitido.

Utilizar ‚Äúkubectl expose deployment <deployment>‚Äù permite crear un servicio directamente.

Ejemplo de manifiesto:

`apiVersion: v1 kind: Service metadata: name: nombre spec: type: Tipo ports: - selector: app: miapp`

### DaemonSet

Tipo de controlador que asegura que exista una copia del pod en cada nodo del cl√∫ster. Sirve para herramientas de monitoreo y networking que necesitan conectar con o saber el estado de los nodos.

La definici√≥n del manifiesto es √≠dentica a la de un ReplicaSet, pero con Kind ‚ÄúDaemonSet‚Äù (lleva un pod template y selectores, sin el campo replicas).

Utiliza _nodeAffinity_ para ubicar los pods.

No existe kubectl create daemonset, por lo que se utiliza kubectl create _ReplicaSet_ o _Deployment_¬† como template y luego se editan los campos necesarios.

El _scheduler_ no administra los pods de un _DS_, sino que lo hace el mismo _Daemonset_, lo cual es modificable en la configuraci√≥n. En caso de modificarlo, el _scheduler_ administra el _DS_ y le cambia el _NodeAffinity_ que trae, para coincidir con la confiugraci√≥n de despliegues pod>nodo que necesita.

En caso de utilizar `--cascade=orphan` al eliminar un _Daemonset_, al desplegar otro con el mismo _Selector_, este adopta los _pods_ que quedaron hu√©rfanos. Luego -si fuera necesario- adecua los pods que no cumplan con el template, recreandolos.

### StatefulSet

Tipo de objeto similar a un _Replica Set_, que permite que cierta cantidad de pods siempre est√© disponible, pero adem√°s tenga las siguientes caracter√≠sticas:

* DNS Hostname fijo

* √çndice ordinal

* Almacenamiento estable, relacionado con esos hostname e √≠ndice

Es una alternativa para aquellas aplicaciones que son _stateful_ y requieren de cierta persistencia a pesar de que los pods roten (se eliminen y recreen). Para obtener esa permanencia, los pods reciben un nombre particular y ordenado, adem√°s de exponerse con un servicio _Headless_ que debe crearse manualmente. El almacenamiento tambi√©n es √∫nico y asociado a cada pod v√≠a nombre, adem√°s de utilizar un _VolumeClaimTemplate_ para crearlo (asumiendo que el cl√∫ster cuenta con _Provisionamiento din√°mico de vol√∫menes_).

El borrado de un _Statefulset_ no implica el borrado de los _PersistentVolumes_ asociados -para conservar datos- y -en ocasiones- puede no asegurar el borrado de los pods (para ello, escalar el _SS_ a 0 y luego eliminar).

### ConfigMap

Objeto que carga datos de aplicaci√≥n como pares de valores _clave:valor_ y es √∫til para inyectar valores a los _containers_ de forma din√°mica (actualizando √∫nicamente el _ConfigMap_).

Se pueden montar como _Enviroment Variables_ (Env) o como _vol√∫menes_.

Al montar como volumen, se crea un archivo por llave y el contenido del archivo es el valor de cada llave.

Ejemplo de manifiesto:

`apiVersion: v1 kind: ConfigMap metadata: name: <nombre> data: llave1: valor1 llave2: valor2`

### Secreto

Objetos peque√±os que almacenan informaci√≥n sensible en texto plano, pero codificados en base 64. Por defecto se almacenan sobre _etcd_ sobre los nodos y pueden montarse como archivos en los pods, que los mantendran sobre memoria (usualmente) para m√°s seguridad.

Se pueden montar igual que los _ConfigMaps_, como _Env_ o como _vol√∫menes_.

Al montar como volumen, se crea un archivo por llave y el contenido del archivo es el valor de cada llave.

#### Niveles de seguridad

Los secretos no son exactamente seguros, ya que se codifican en base 64 y son facilmente decodificables. A√∫n as√≠, Kubernetes aplica estrategias de protecci√≥n:

* El Secreto existe en un nodo, solo durante la vida util del pod que lo invoca.

* El secreto se monta sobre tmpfs y no se escribe en disco (al montar como volumen)

Notas de color:

* Cualquiera con permisos GET de secretos puede leer el contenido (y decodificarlo).

* Dentro del pod el secreto no est√° ofuscado, por lo que cualquiera con acceso al pod puede obtener el valor del secreto.

Se recomienda activar Encryption at rest para ETCD, asegurando que nadie puede leer el contenido de los mismos si se vulera ETCD.

Ejemplo de manifiesto:

`apiVersion: v1 kind: Secret metadata: name: <nombre> data: llave1: <valorEncodeadoEnBase64>`

#### Codificar/decodificar en Base 64

Sobre Linux se puede utilizar base64 para codificar/decodificar:

`echo -n <string> | base64`

![:mag:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f50d.png) Observabilidad
=======================================================================================================================================================

Monitoreo
---------

No existen soluciones nativas de monitoreo en Kubernetes. Las herramientas de 3ros mas comunes son:

\-Metrics servers

\-Prometheus

#### Metric Server (Ex Heapster)

El componente ‚Äúcadvisor‚Äù (Container advisor) le expone m√©tricas al Kubelet de cada nodo, que luego lo consume el Metric Server.

##### Comandos

\-Metricas de nodos: `k top node`

\-Metricas de pods: `k top pods`

Open

Logging
-------

Los logs que muestra _Kubernetes_ son los que se emiten desde los containers en los streams de STDOUT y STDERR.

Como los _nodos_ son quienes alojan los pods -y estos los contianers-, es en su almacenamiento local que se guardan los logs, usualmente en el path ‚Äú_/host/var/log/containers/<NombreCointaner>.log_

Usualmente se despliegan herramientas de logging como _Daemonset_, donde un pod por nodo se encarga de recolectar esos logs, parsearlos y enviarlos a una soluci√≥n de logging general (no necesariamente externa a K8s).

![:hammer:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f528.png) Mantenimiento del cl√∫ster
=====================================================================================================================================================================

Mantenimiento de nodos
----------------------

Cuando un nodo se encuentra _offline_ inicia un conteo llamado _pod-eviction-timeout_, que por defecto es de 5 minutos. Luego de ese tiempo, el _Master_ considera a los pods _terminated_ y -por m√°s que el nodo vuelva a estar operativo- los saca de ese nodo.

### Operaciones

#### Drain

Terminar los pods asignados al nodo y re-asignarlos a otros. Adem√°s, marca al nodo con _cordon_ (no disponible para recibir cargas de trabajo) y luego debe ser marcado como _uncordoned_ para que vuelva a recibir pods.

`kubectl drain <nodo> --ignore-daemonsets`

Los pods que no son parte de un _replicaset_ pueden generar problemas y debe agregarse `--force`, ya que los mismos pierden disponibilidad al ser _evicted_ y no se crean autom√°ticamente en otros nodos.

#### Cordon

Marcar como _unschedulable_ (no disponible para recibir cargas de trabajo) a un nodo, para que no se le puedan asignar m√°s pods. Luego debe marcarse como _uncordoned_ para que vuelva a recibir pods.

`kubectl cordon <nodo>`

`kubectl uncordon <nodo>`

![:cd:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4bf.png) Actualizaci√≥n del cl√∫ster
=================================================================================================================================================================

Versionado de Kubernetes
------------------------

El formato de las versiones de Kubernetes es el siguiente:

v <major>.<minor>.<patch>

Ejemplo: v1.11.3 corresponde a la versi√≥n major 1, minor 11 y parche 3.

Los _minor releases_ traen nuevas funciones y caracter√≠sticas; los parches arreglan bugs.

Primera versi√≥n: 1.0 en Julio de 2015.

Adem√°s, las versiones pasan de Alpha a Beta y luego a un release (Alpha>Beta>Release).

Planeamiento previo
-------------------

El contenido de un release incluye: _kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy y kubectl_ (componentes principales).

Luego ETCD cluster, CoreDNS y otros componentes de sistema se actualizan aparte y no comparten el versionado.

El _Kube-APIServer_ es el que debe tener la versi√≥n mayor, ya que todos los componentes se comunican con dicho componente. Luego pueden existir diferencias de _release-version_ hacia atr√°s:

\-1 versi√≥n menos: _Kube-Controller-Manager_ y _Kube-Scheduler_ (_cluster-wide components_)

\-2 versiones menos: _Kubelet_ y _Kube-Proxy (node-side_ components)

Solo _Kubectl_ puede tener una versi√≥n mayor (o menor) que _Kube-APIServer_, porque eso permite hacer live-upgrades del cluster.

El soporte de Kubernetes incluye un rango de las √∫ltimas 3 versiones, incluida la actual.

C√≥mo actualizar
---------------

Las actualizaciones se hacen de a un _minor_ release a la vez.

Se actualiza primero el _Master_ y luego los _worker Nodes_. Por ende, por unos minutos las funcionalidades de management no estar√°n activas (dependen del master) , pero si las cargas de trabajo seguiran vivas. Luego, idealmente se deben actualizar los nodos de _forma escalonada_, aunque se pueden hacer todos en paralelo (con downtime):

1. Actualizar _Master/s_ o _Control Plane_/s

2. Actualizar _Nodes_ de forma escalonada, con estrategias:

    1. Actualizar _Nodos_ uno a uno.

    2. Reemplazar _Nodos_ uno a uno, con nuevos nodos que tengan una imagen nueva.

En todos los casos, la actualizaci√≥n de un nodo implica la alteraci√≥n de las cargas de trabajo, por lo que deben desalojarse con operaciones _drain_ y _cordon_.

### Actualizar con Kubeadm

#### Vista previa

En lineas generales, el flujo de upgrade implica:

1. Sobre el _Master_:

    1. Actualizaci√≥n de Kubeadm

    2. Plan de upgrade

    3. Aplicacion de Upgrade

    4. Actualizaci√≥n de _Kubectl_ y _Kubelet_

    5. Reinicio de _Kubelet_

2. Sobre los _Nodes_:

    1. Aplicaci√≥n de Upgrade

    2. Actualizaci√≥n de _Kubelet_ y _Kubectl_

    3. Reinicio de _Kubelet_

N√≥tese que no actualiza los _Kubelet_ o _Kubectl_ de los nodos, sino que es una tarea manual. Por ende, es normal luego de un upgrade con _Kubeadm_ ver una versi√≥n ‚Äúanterior‚Äù al ejecutar el comando `kubectl get nodes`, ya que eso muestra la versi√≥n del _Kubelet_, no alcanzado por el upgrade.

Si los _Master Nodes_ tuvieran _Kubelet_ y alojaran cargas de trabajo, debe aplicarse adem√°s la estrategia de upgrade de los _worker Nodes_ luego de actualizar.

Seguir guia de:

[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/) ¬†

#### Comandos

**Master Node/Control Plane**

`#Actualizar Kubeadm apt-mark unhold kubeadm kubelet kubectl apt-get update apt install -y kubeadm=1.xx.x-00 #Planear upgrade kubeadm upgrade plan #Aplicar upgrade con Kubeadm: kubeadm upgrade apply v1.xx.x #Actualizar Kubectl y Kubelet (si aloja workloads) apt install -y kubelet=1.xx.x-00 kubectl=1.xx.x-00 apt-mark hold kubeadm kubelet kubectl #Reiniciar Kubelet (si aloja workloads) systemctl daemon-reload systemctl restart kubelet systemctl status kubelet`

**Worker Node**

`#Desalojar cargas de trabajo del nodo: kubectl drain <nodo> #Obtener updates y aplicar upgrades sobre Kubeadm apt-mark unhold kubeadm kubelet kubectl apt-get update apt install -y kubeadm=1.xx.x-00 #Aplicar upgrade kubeadm upgrade node #Actualizar Kubectl y Kubelet apt install -y kubelet=1.xx.x-00 kubectl=1.xx.x-00 apt-mark hold kubeadm kubelet kubectl #Reiniciar Kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet #Activar nodo para cargas de trabajo: kubectl uncordon <nodo>`

Notese la diferencia en el comando `Kubeadm Upgrade`, que en los nodos Workers no aclara versi√≥n, ya que se utiliza la del master.

![:floppy_disk:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4be.png) Respaldo/Backup del cl√∫ser
===========================================================================================================================================================================

Respaldo de Manifiestos
-----------------------

Forma rudimentaria de ‚Äúextraer‚Äù todo el contenido de un cluster:

`kubectl get all --all-namespaces -o yaml > archivo.yaml`

Esto es lo que hace la herramienta de respaldo _Velero_ (ex _Heptio Ark_).

Adem√°s, mantener un repositorio _GIT_ con _manifiestos_ es la mejor forma de tener un respaldo a la √∫ltima version de los objetos desplegados en _K8s._

Respaldo de ETCD
----------------

`#Version de ETCDCTL API export ETCDCTL_API=3 #Respaldo. No omitir par√°metros de certificados, configuraciones, etc. etcdctl snapshot save ./snapshot.db --endpoints https://127.0.0.1:2379 --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --cacert /etc/kubernetes/pki/etcd/ca.crt #Para validar el estado del snapshot, utilizar etcdctl snapshot status ./snapshot.db --write-out=json --endpoints https://127.0.0.1:2379 --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --cacert /etc/kubernetes/pki/etcd/ca.crt #Restaurar. Esto inicia una configuraci√≥n de cl√∫ster ETCD nueva con ese path. systemctl stop kube-apiserver systemctl stop etcd etcdctl snapshot restore ./snapshot.db --data-dir /var/lib/etcd-from-bkp¬† systemctl daemon-reload systemctl start kube-apiserver systemctl start etcd`

Si _ETCD_ corre como _static pod_, debe actualizarse el YAML de configuraci√≥n para que apunte el nuevo path y forzarse reinicio de _Kubelet_ en el nodo para que actualice.

[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)

![:key:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f511.png) Seguridad
==================================================================================================================================================

Nodos
-----

Idealmente deber√≠an tener:

* Acceso root deshabilitado

* Password-based authentication deshabilitada

* Acceso mediante SSH Keys

Comunicaci√≥n entre componentes
------------------------------

Debe utilizar _encripci√≥n TLS_ punta a punta, con certificados aprobados por un _Certification Authority_ (_CA_).

En general, el _CA_ es el _Master Node_, que luego debe aprobar los _CertificateSigningRequests_ de los diferentes componentes, para que cada uno tenga sus pares de certificados v√°lidos.

Mecanismos de seguridad en un cl√∫ster
-------------------------------------

### Autenticaci√≥n/Authentication

_**¬øQui√©n puede acceder?**_

\-P√∫blico: Usuarios administrativos (Administradores y Developers) y aplicaciones/servers (Service Accounts).

El componente que autentica es el _Kube-APIServer_, utilizando una de estas opciones:

* **Static Password File:** CSV con columnas ‚Äúpwd,usuario,userid,grupos‚Äù enviado como par√°metro `--basic-auth-file` al encender el servicio _Kube-APIServer_.

* **Static Token File:** Igual que el anterior, un CSV pero con Tokens en lugar de passwords, enviado como par√°metro `--token-auth-file` al encender el servicio _Kube-APIServer_.

* **Certificates:** Certificados _TLS_.

* **Identity Services externos**: Integraci√≥n con alg√∫n servicio externo.

De m√°s est√° decir que los m√©todos de archivos est√°ticos no son recomendados por cuestiones de seguridad y administrabilidad.

Deben acompa√±arse los usuarios con Roles y Role Bindings necesarios.

* Para un llamado API con autenticaci√≥n b√°sica, se agrega el par√°metro `-u <user>:<password>.`

* Para un llamado API con autenticaci√≥n b√°sica de token, se agrega el par√°metro `--header "Authorization: Bearer <token>"`.

### Autorizaci√≥n/Authorization

_**¬øQu√© puede hacer?**_

La autorizaci√≥n es el mecanismo de seguridad que -luego de la autenticaci√≥n- indica qu√© tareas puede realizar o a qu√© recursos puede acceder un _usuario_ o _identidad_; los _roles_ definen una serie de permisos sobre operaciones que se pueden realizar en diferentes _API Groups_ (grupos _API_ que engloban operaciones con ciertos tipos de recursos). Luego las asociaciones de roles (_Role Bindings_) asignan un rol a un usuario/grupo.

#### Mecanismos de autorizaci√≥n

\-**AlwaysAllow** (Default): Aprueba todos los requests.

\-**AlwaysDeny**: Deniega todos los requests.

\-**Node Authorizer**: Solo autoriza nodos, mediante el grupo `system:nodes`.

\-**ABAC** (Attribute-Based Access Control).

\-**RBAC** (Role-Based Access Control).

\-**Webhook**: Integraci√≥n y delegaci√≥n de autorizaciones a un servicio externo, mediante Webhook HTTP.

El par√°metro `--authorization-mode` enviado al iniciar el `Kube-APIServer` indica el/los mecanismos de autorizaci√≥n a utilizar en el cl√∫ster. Pueden adem√°s utilizarse de forma combinada, especificandolos separados por coma en el par√°metro `--authorization-mode`. De esta forma, corren en el orden indicado en la lista, y se van pasando el request (solo en caso de denegarlo).

#### ABAP

Se asocia un grupo (o grupo de usuarios) a un permiso espec√≠fico, sucesivamente hasta cubrir todas las operaciones que se aprueban para el usuario. Luego cada usuario tiene su grupo de entradas, las cuales se incrementan cuando se incrementa la cantidad de usuarios.

Agregar una l√≠nea implica reiniciar el Kube-APIServer. Por ende, este m√©todo no es recomendado para un cl√∫ster de gran escala, ya que es dificil de administrar.

Ejemplo de permiso (cualquier operaci√≥n, sobre cualquier recurso):

`{"apiVersion": "abac.authorization.kubernetes.io/v1beta1", "kind": "Policy", "spec": {"user": "alice", "namespace": "*", "resource": "*", "apiGroup": "*"}}`

#### RBAC

Implica la definici√≥n de roles con cierta cantidad de permisos asociados y luego asignar dichos roles a un usuario/grupo de usuarios. De esta forma, el cambio -al incrementar o agregar permisos- ocurre sobre el rol y aplica para todos los usuarios que lo tienen (ahorrando operaciones sobre cada usuario).

Los roles tienen dos alcances:

* **Cluster**: Tanto para aquellos recursos que **no** son _Namespaced (Cluster-Wide)_ como para aquellos que **si** son _Namespaced_. Estos permisos se crean con roles llamados _ClusterRole_ y afectan a nivel cl√∫ster.

* **Namespace**: Para aquellos recursos que **si** son _Namespaced_. Se crean con roles llamados _Roles_ y afectan a nivel namespace.

Los permisos son aditivos y no existen reglas de denegaci√≥n. Por ende, es importante definir el rol completo, o el usuario solo tendr√° acceso a aquellos recursos que el mismo defina.

Los roles se pueden definir a nivel namespace, utilizando _metadata.namespace_ y hacer efecto solo en ese NS.

##### Comandos

Obtener roles:

`kubectl get roles`

Obtener asignaciones de roles:

`kubectl get rolebindings`

Crear un rol o clusterRole:

`kubectl create cluster/role <Nombre> --verb=<Verbo1>,<Verbo2> --resource=<RecursoOAPIGroup>`

Crear un rolBinding o ClusterRoleBinding (asociar un rol) a un usuario:

`kubectl create cluster/rolebinding <Nombre> cluster/role=<NombreRol> --serviceaccount=<Namespace>:<Nombre>`

Crear un rolBinding o ClusterRoleBinding (asociar un rol) a una ServiceAccount:

`kubectl create cluster/rolebinding <Nombre> cluster/role=<NombreRol> --user=<Username>`

Ver si el usuario actual tiene permisos para hacer una operaci√≥n:

`kubectl auth can-i <verbo> <recurso>`

Ver si se puede hacer una operaci√≥n pero como otro usuario:

`kubectl auth can-i <verbo> <recurso> --as <usuario>`

Para ver si un _ServiceAccount_ puede hacer una operaci√≥n:

`kubectl auth can-i <verbo> <recurso> --as system:serviceaccount:<Namespace>:<Nombre>`

##### Role definition

Define las tareas que puede realizar un rol.

`apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:¬† name: developer rules:¬† - apiGroups: [""]¬† ¬† resources: ["pods"]¬† ¬† verbs: ["list","get","create","update","delete"] ¬†¬†¬†¬†#Opcional para restringir por recurso especifico ¬†¬†¬†¬†#resourceNames: [‚Äúpod1‚Äù,‚Äùpod2‚Äù]¬† - apiGroups: [""]¬† ¬† resources: ["ConfigMap"]¬† ¬† verbs: ["create"]`

Para objetos del grupo ‚ÄúCore‚Äù, no hace falta definir el APIGroup (usar ‚Äú‚Äù).

Ejemplo de permiso ‚ÄúAny-Any‚Äù o Cero-Restrictivo (acceso a todo en el Namespace):

`apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:¬† name: developer rules:¬† - apiGroups: [‚Äú*‚Äù]¬† ¬† resources: [‚Äú*‚Äù]¬† ¬† verbs: [‚Äú*‚Äù]`

##### Role binding

Asocia un rol a un usuario o identidad (puede ser un grupo).

`apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata:¬† name: devuser-developer-binding subjects:¬† - kind: User¬† ¬† name: dev-user¬† ¬† apiGroup: rbac.authorization.k8s.io roleRef:¬† kind: Role¬† name: developer¬† apiGroup: rbac.authorization.k8s.io`

N√≥tese que los APIGroups no tienen el V1 y que el campo _roleRef_ **no** es un _Array_.

##### ClusterRole

Por defecto, aplica sobre recuros _ClusterScope_ (globales). En caso de asociar un ClusterRole a un recurso Namespaced, eso otorga el mismo rol sobre todos los namespaces.

El recurso _ClusterRole_ en s√≠ no es parte de ning√∫n _Namespace_, al igual que un _nodo_, por ser recursos _ClusterScope_ o _ClusterWide_.

`apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:¬† name: developer rules:¬† - apiGroups: [""]¬† ¬† resources: ["nodes"]¬† ¬† verbs: ["list","get","create","update","delete"] ¬†¬†¬†¬†#Opcional para definir sobre un recurso especifico ¬†¬†¬†¬†#resourceNames: [‚Äúnodo1‚Äù]`

##### Cluster Role binding

Asocia un _ClusterRole_ a un usuario.

`apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:¬† name: devuser-developer-binding subjects:¬† - kind: User¬† ¬† name: dev-user¬† ¬† apiGroup: rbac.authorization.k8s.io roleRef:¬† kind: ClusterRole¬† name: developer¬† apiGroup: rbac.authorization.k8s.io`

### Service Account

Es una cuenta utilizada por servicios o software. No lleva clave, sino que utiliza un _Bearer Token_, cuyo valor se almacena como _secreto_ y se asocia al objeto _ServiceAccount_.

Para utilizarlo, en general se monta como volumen en el _pod_, para disponibilizar el _token_ dentro del mismo. Cada _Namespace_ tiene un _ServiceAccount_ llamado ‚ÄúDefault‚Äù. Estos se montan de forma autom√°tica en el Pod, sobre /var/run/secrets/kubernetes.io (donde tambi√©n reside el CA.cert).

No se ven en el _deployment_ si es el _token_ montado por defecto, a no ser que se especifique con el campo _serviceAccount_.

La propiedad _automountServiceAccountToken_ permite desactivar esta funcionalidad, con el valor ‚Äúfalse‚Äù.

El formato de las cuentas es `"system:<TipoDeCuenta>:<Namespace>:<Cuenta>"`.

#### Comandos

Crear _ServiceAccount_:

`kubectl create serviceaccount <nombre>`

Comunicarse con API mediante uso de cuenta:

`curl https://localhost:6443/api -insecure --header "Authorization: Bearer <ValorDeToken>"`

Complementario: Certificados TLS
--------------------------------

Un certificado es utilizado para asegurar una conexi√≥n cifrada de confianza entre dos servidores. Para descifrar los datos encriptados, es necesaria una llave (usualmente como representada como _Certificado_).

### Encripci√≥n sim√©trica

Utiliza la misma llave para encriptar/desencriptar los datos, por lo cual es vulnerable (la llave debe viajar de forma p√∫blica y puede ser interceptada).

Open

### Encripci√≥n asim√©trica

Se utilizan dos llaves, una _privada_ y una _p√∫blica_ -o _lock_\-.

La _llave p√∫blica_ solo sirve para _cifrar_ el contenido, no _descifrarlo_.

Ambas llaves pueden encriptar datos, pero solo la _privada_ desencriptarlo, por lo cual es importante nunca encriptar con la clave privada, ya que eso permitir√≠a desencriptar con la clave p√∫blica (invertir los roles).

Usualmente las llaves p√∫blicas tienen extensi√≥n _CRT_ o _PEM_, mientras que las privadas tienen extensi√≥n _KEY_ (o _PEM_ con el sufijo _\-key_ en el nombre). Son archivos de texto plano que dentro tienen el contenido del certificado encriptado.

Open

#### Llaves SSH en Linux

Se utiliza `ssh-keygen` para generar par de llaves y luego ubicarlas en _/.ssh/authorized\_keys_.

#### Llaves para HTTP

Se utiliza `openssl genrsa -out clave.key 1024` para generar la llave _privada_ y _o_`penssl rsa -in clave.key -pubout > lock.pem` para generar clave _p√∫blica_ (o _lock_).

### Combinaci√≥n de encripci√≥n sim√©trica y asim√©trica

Se env√≠a una copia de la llave _p√∫blica_ **asim√©trica** al cliente -quien solo puede encriptar informaci√≥n con ella\- y este la utiliza para encriptar la llave _privada_ **sim√©trica**. Luego el servidor recibe ese paquete encriptado, lo desencripta con la llave _privada_ **asim√©trica** (√∫nica llave que puede desencriptarlo) y as√≠ puede recibir/visualizar el contenido de la llave _privada_ **sim√©trica**. Luego comienzan a enviar datos encriptados sim√©tricamente, que solo pueden ser desencriptados por la llave privada sim√©trica, pero que solo tienen ellos en su poder y no hace falta enviar v√≠a internet.

La clave p√∫blica se env√≠a como certificado.

Resumen de flujo:

1. El servidor env√≠a llave p√∫blica asimetrica

2. El cliente encripta su llave privada sim√©trica con la llave p√∫blica asim√©trica

3. El servidor recibe el paquete encriptado y lo desencripta con la llave privada asim√©trica

4. Se establece un canal de conexi√≥n seguro por encripci√≥n sim√©trica, donde la llave privada sim√©trica solo vive en los extremos cliente y servidor

En definitiva, se utiliza el m√©todo _asim√©trico_ de encripci√≥n para poder intercambiar claves y luego poder adoptar el m√©todo _sim√©trico_ de encripci√≥n.

### Uso de certificados - Public Key Infrastructure (PKI)

Un _certificado TLS_ se utiliza para enviar dentro una _llave p√∫blica_, pero acompa√±arla con informaci√≥n relevante sobre la misma, su portador, su emisor (o firmante) y el nombre del sitio -CN- (o URL).

#### Autoridad certificante (Certification Authority)

Si el dominio del sitio no coincide con el declarado en el certificado, puede implicar un intento de phishing; si el firmante no es de confianza, puede implicar un intento de hackeo. Por ende, se recurre a un firmante/emisor (signer o issuer) que tenga autoridad, lo cual se llama _Certficitation Authority_ (_CA_), que introduce los _Root certificates_ (o certificados raiz).

Para obtener un certificado firmado para una llave p√∫blica, se crea un _Certificate Signing Request_ (o _CSR_) para esa llave y dominio, luego se env√≠a al _firmante_ quien valida la informaci√≥n y env√≠a el certificado firmado (que goza de _Trust_ (o confianza) por la validez del firmante.

Luego, para certificar esta confianza, los _CA_ utilizan un par de llaves asim√©tricas, donde la clave p√∫blica la carga el navegador y la clave privada la utiliza el _CA_ para firmar un certificado.

Para uso interno en una compa√±ia, se puede crear un _CA privado_, luego instalar la llave p√∫blica en todos los browsers de los empleados y firmar certificados con la llave privada de dicho CA, para garantizar validez.

Tambi√©n puede exigirse un certificado de cliente, para que el server valide confianza con el mismo, lo cual se hace tambi√©n con un CSR a un trusted CA, luego enviando la info. al servidor.

### Uso en Kubernetes

Todos los componentes _cliente_ tienen su propio par de llaves privada/p√∫blica y los usan para comunicarse con los servicios de tipo _server_, que aseguran esos servicios con otro par de llaves propio. Luego debe existir un _CA_ para todos esos certificados, con el que todos establezcan confianza (deben tener instalado el cert. p√∫blico del CA).

#### Comandos TLS para Linux

##### Generar certificado autofirmado para CA

`#Generar llave privada openssl genrsa -out <llave>.key 2048 #Generar CSR (Request) para esa llave openssl req -new -key <llave>.key -subj "/CN=<Nombre>" -out <Request>.csr #Firmar certificado (propio CA) openssl x509 -req -in <Request>.csr -signkey <llave>.key -out <Certificado>.crt`

La llave p√∫blica va en el certificado firmado.

##### Generar certificado para usuarios (Admins)

`#Generar llave privada openssl genrsa -out <llave>.key 2048 #Generar CSR (Request) para esa llave openssl req -new -key <llave>.key -subj "/CN=<Nombre>/O=system:<usuarios>" -out <Request>.csr #Firmar certificado (en el CA!) openssl x509 -req -in <Request>.csr -signkey <llaveCA>.key -out <Certificado>.crt`

##### Generar certificado para componentes del sistema

`#Generar llave privada openssl genrsa -out <llave>.key 2048 #Generar CSR (Request) para esa llave openssl req -new -key <llave>.key -subj "/CN=system:<Nombre>" -out <Request>.csr #Firmar certificado (en el CA!) openssl x509 -req -in <Request>.csr -signkey <llaveCA>.key -out <Certificado>.crt`

##### Ver detalles de un certificado

`openssl x509 -in <certificado>.crt -text -noout #O bien -solo para ver el certificado, pero encriptado- cat <certificado>.crt`

##### Ver detalles de un CertificateSigningRequest

`openssl req -text -noout -verify -in CSR.csr`

##### Llamado API con certificados

Usando un comando:

`curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt`

Usando kube-config:

`apiVersion: v1 kind: Config clusters:¬† - cluster:¬† ¬† ¬† certificate-authority: ca.crt server: https://kube-apiserver:6443 name: kubernetes users: - name: kubernetes-admin user:¬† ¬† ¬† client-certificate: admin.crt¬† ¬† ¬† client-key: admin.key`

### Certificado de Kube-APIServer

Debe incluir los nombres:

* kubernetes

* kubernetes.default

* kubernetes.default.svc

* kubernetes.default.svc.cluster.local

Para generarlo, primero armar un archivo de configuraci√≥n de _OpenSSL_:

`[req] req_extensions = v3_req [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation,subjectAltName = @alt_names [alt_names] DNS.1=kubernetes DNS.2=kubernetes.default DNS.3=kubernetes.default.svc DNS.4=kubernetes.default.svc.cluster.local IP-1=10.96.0.1 IP-2=172.17.0.87`

Y luego crear request con ese archivo como par√°metro:

`openssl req -new -key <llave>.key -subj "/CN=kube-apiserver" -out <request>.csr -config openssl.cnf`

Luego firmar con el _CA_ y especificarlo en la linea de inicio del _Kube-APIServer_ como par√°metro.

##### Certificados de Nodos o Kubeletes

Son pr√°cticamente _HTTP Servers_, por lo que llevan su propio par de llaves firmado por el _CA_. El _CN_ utilizado es el _hostname_ del nodo.

Luego se configuran esos certificados como par√°metros en el _Kubelet_ config file. Adem√°s, para autenticar contra el _Kube-APIServer_, lleva un par de llaves de cliente, cuyos certificados llevan el _CN_ con sufijo `system:node:<nombrenodo>`, para categorizarlos y recibir los permisos correctos.

Los certificados en un cl√∫ster existente pueden identificarse en la l√≠nea de comandos que inicia el _Kube-APIServer_, que puede o no ser un _static pod_.

#### Estado de salud TLS de un cl√∫ster

Para chequear el estado de salud de un cl√∫ster y sus certificados:

1. Asegurarse de que los _CN_ y _ALT_ names son correctos para todas las emisiones de certificado.

2. Verificar que no hayan expirado los certificados

3. Verificar que el _CA_ firmante es el correcto

4. Verificar que el certificado _CA_ est√© instalado en todos los servidores, para asegurar confianza

5. Verificar que la ruta de los archivos de certificado es la correcta

### Uso en ETCD

_ETCD_ opera como un cl√∫ster con un servidor central, por lo que tiene sus propios pares de llaves de servidor y de clientes (en este caso, los _peers_). Puede adem√°s tener su propio _CA_, o compartir el del _master_ de _Kubernetes_.

### Certificates API

_API_ de _Kubernetes_ que permite operar en nombre del _CA_ y ver/aprobar/firmar _requests_ (o listar certificados); est√° alojada dentro del Kube-_Controller-Manager_.

Los par√°metros `--cluster-signing-cert-file` y `--cluster-signing-key-file` enviados al _Kube-Controller-Manager_ al iniciar indican qu√© par de claves utilizar para aprobar como _CA_.

Para solicitar un _CSR_, se crea un objeto de _K8s_ llamado _CertificateSigningRequest_:

`apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: nombre spec: usages:¬† ¬† ¬† ¬† - digital signature¬† ¬† ¬† ¬† - key encipherment¬† ¬† ¬† ¬† - server auth clientAuthRequest: <Contenido de archivo CSR Codificado en Base64> ¬†¬†signerName: kubernetes.io/kube-apiserver-client`

#### Crear un usuario nuevo

Crear llave _privada_ y _CSR_:

`openssl genrsa -out myuser.key 2048 openssl req -new -key myuser.key -out myuser.csr`

Encodear en base 64 el _CSR_ y preparar archivo _YAML_:

`cat myuser.csr | base64 > myusercsr.yaml`

Armar manifiesto _YAML_ con ese _CSR_ codificado:

`apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: myuser spec: request: <CSREncodeadoEnBase64> signerName: kubernetes.io/kube-apiserver-client usages: - client auth`

Aplicar el manifiesto con `kubectl apply -f myusercsr.yaml` y luego buscar con `kubectl get cs`r myusercsr.

Para aprobar, ejecutar `kubectl approve certificate myusercsr` y luego obtener el valor del certificado, decodificarlo en base 64 y guardarlo:

`kubectl get csr myusercsr -o jsonpath='{.status.certificate}' | base64 -d | myusr.crt`

Para asignarle un _rol_ no hace falta el _certificado_ en si, solo el nombre. En cambio para que el usuario se conecte, debe utilizar ambos _certificado_ y _llave privada_ en su _kubeconfig_ al conectarse al cl√∫ster.

[https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#normal-user)

#### Comandos de Certificates API

Ver _CSR_:

`kubectl get csr`
Aprobar _CSR_:

`kubectl certificate approve <nombre>`
Denegar _CSR_

`kubectl certificate deny <nombre>`

El certificado generado una vez aprobado el _CSR_ se puede ver en el _manifiesto_ propio del _CSR_ como _YAML_, secci√≥n _Status.certificate_ (codificado en base64).

### Configuraci√≥n de Kubectl - Kubeconfig

As√≠ como debe autorizarse un llamado _API_ al _Kube-APIServer_, deben autenticarse los comandos de _Kubectl_.

Si bien podr√≠an especificarse los par√°metros manualmente en cada comando, por ejemplo:

`kubectl get pods --server <servidor> --client-key <llave> --client-certificate <cert> --certificate-authority <CACert>`

Esto ser√≠a algo tedioso, por lo que se utiliza un archivo denominado _Kubeconfig_, que contiene esos par√°metros:

`--server <servidor> --client-key <llave> --client-certificate <cert> --certificate-authority <CACert>`

#### Ubicaci√≥n del archivo

Por defecto, _Kubectl_ espera este archivo ubicado en _$HOME/.kube/config_ (_%homepath%\\.kube\\config_ en Windows).

En Linux los directorios que tienen el prefijo ‚Äú.‚Äù son ocultos, por lo que se debe agregar el par√°metro ‚Äú-a‚Äù a ‚Äúls‚Äù para poder ver la carpeta.

#### Contenido del archivo

Tiene tres secciones:

* Clusters: Configuraci√≥n de cl√∫sters utilizados.

* Contexts: Asocia un cluster a un usuario.

* Users: Usuarios utilizados para login (aqu√≠ van los certificados).

El formato utilizado es _YAML_:

`apiVersion: v1 kind: Configclusters: name: <MiCluster1>¬† cluster: ¬†¬†certificate-authority: <CACert> ¬†¬†#Opcional ¬†¬†#certificate-authority-data: <ContenidoCertEnBase64> ¬†¬†server: <https://MiCluster1:6443/> contexts: name: <MiUsuario1>@<MiCluster1> ¬†¬†¬†¬†context:¬† ¬†¬†¬†¬†¬†¬†cluster: <MiCluster1> ¬†¬†¬†¬†¬†¬†user: <MiUsuario1> ¬†¬†¬†¬†¬†¬†#Opcional ¬†¬†¬†¬†¬†¬†#namespace: <MiNamespace1> users: name: <MiUsuario1> ¬†¬†¬†¬†user:¬† ¬†¬†¬†¬†¬†¬†client-Certificate: <CertDeUsuario> ¬†¬†¬†¬†¬†¬†client-key: <LlaveDeUsuario>current-context: "<MiUsuario1>@<MiCluster1>"`

* Para ver el contenido usar el comando `kubectl config view`

* Para elegir un contexto utilizar `kubectl config use-context <NombreContexto>` (agregar parametro `--kubeconfig <archivo>` para utilizar otro archivo)

* Para setear un parametro utilizar `kubectl config set <seccion.parametroOItem.Propiedad> <valor>`

* Para ver el contexto actualmente utilizado, usar `kubectl config current-context`

Encryption at rest
------------------

_Kubernetes_ utiliza _Data encryption at rest_ para los _secrets_ nativos, para lo cual utiliza una _llave de encripci√≥n_ que permite ofuscar el valor de la informaci√≥n. Luego el par√°metro `--encryption-provider-config` enviado al _Kube-APIServer_ al iniciarlo con su binario es el que monta el un _manifiesto_ que contiene esta llave.

**Generar llave de encripci√≥n**

`ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)`

**Manifiesto de EncryptionConfig**

`apiVersion: v1 kind: EncryptionConfig resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: ${ENCRYPTION_KEY} - identity: {}`

API Groups
----------

A alto nivel, los recursos se diferencian entre aquellos que son _Core_ (nucleo, nativas) y aquellos que son _Named_ (funcionalidad extendida). Los recursos core se acceden mediante el _URL Path_ /api, mientras que las named con el _path_ /apis; dentro de estos grupos existen versiones como v1, v1beta, etc.

Luego, cada _Recurso_ (u _objeto_) dentro de un grupo tiene sus propios _verbos_ (o _m√©todos_) que indican qu√© operaciones se pueden realizar con el mismo.

### Core

* Namespaces

* Pods

* events

* PV

* PVC

### Named

* /apps

* /extensions

* /networking.k8s.io

* /storage.k8s.io

* /authentication.k8s.io

* /certificates.k8s.io

### Otras consultas API

Obtener versi√≥n de _Kubernetes_:

`/version`

M√©tricas:

`/metrics`

Salud:

`/healthz`

Logs:

`/logs`

Mostrar todos los paths:

`/`

### Authenticaci√≥n API con Kubectl Proxy

La funcionalidad _proxy_ de _kubectl_ reenvia el tr√°fico del localhost al cl√∫ster y le alimenta las credenciales del archivo _Kubeconfig_, ahorrandose el uso de par√°metros de authenticaci√≥n para comandos API ejecutados durante la ejecuci√≥n del proxy.

`kubectl proxy & curl https://localhost:6443/`

Image Security
--------------

El path para las _imagenes_ de contenedores es:

_<ImageRegistry>/<UserAccount>/<Repositorio:Imagen>_

Repositorios conocidos:

\-Google: [http://gcr.io](http://gcr.io)

\-Docker: [http://docker.io](http://docker.io)

La _UserAccount_ por defecto es _Library_, que se utiliza al no declarar ninguna.

#### Repositorio Privado

Autentica con un secreto de tipo _docker-registry_, que contiene el _servidor_ y las _credenciales_ de acceso al _repositorio_. Luego, el campo _spec.imagePullSecrets_ dentro del template del _pod_ indica qu√© secreto utilizar para traer una imagen del repositorio:

`kubectl create secret docker-registry regcred --docker-server=<servidor> --docker-username=<usuario> --docker-password=<clave> --docker-email=<correo>`

Security Contexts
-----------------

Los _contextos de seguridad_ de **Linux** sirven para definir reglas de comportamiento del sistema de archivoso el propio _SO_. En _Kubernetes_ se pueden aplicar a nivel _pod_ (aplica a todos los _containers_), a nivel _container_ o en ambos (_pods_ y _containers_ tienen su propia configuraci√≥n). Algunos casos particulares tambi√©n se pueden aplicar a vol√∫menes.

Alungos _SC_ solo aplican a nivel _pod_.

Para aplicar el contexto, se agrega lo siguiente sobre la secci√≥n _spec_ (sea del pod o del container): `securityContext:¬† runAsUser: 1000`

Ejemplo de un caso con SC distintos a nivel POD y container:

`spec:¬† securityContext: runAsUser: 1001¬† containers:¬† -¬† image: ubuntu ¬† ¬† name: web ¬† ¬† command: ["sleep", "5000"] ¬† ¬† securityContext: runAsUser: 1002¬† -¬† image: ubuntu ¬† ¬† name: sidecar ¬† ¬† command: ["sleep", "5000"]`

El pod utiliza el usuario con ID 1001, al igual que el container ‚Äúsidecar‚Äù porque lo hereda. Por otro lado, el container nombrado ‚Äúweb‚Äù utiliza el usuario id 1002, declarado de forma impl√≠cita en su _spec._

El ID del usuario Root es 0.

##### Capabilities

Las _capabilities_ de **Linux** permiten otorgar ciertos permisos a un proceso, sin necesariamente darle permisos _root_.

Solo est√°n disponibles a nivel _container_, agregandolas sobre la secci√≥n _spec_:

`securityContext:¬† runAsUser: 1000 capabilities:¬† ¬† add: ["MAC_ADMIN"]`

M√°s info. en [https://kubernetes.io/docs/tasks/configure-pod-container/security-context/](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).

Network Policy
--------------

* **Egress:** Tr√°fico externo

* **Ingress:** Tr√°fico interno

_Objeto_ que permite administrar el tr√°fico que recibe/env√≠a un _pod_. Se asocia mediante el uso de _selectores_ de _labels_, al igual que un _ReplicaSet_ o un _Service_.

Por defecto, no existen restricciones de _Ingress_/_Egress_ a los _pods_, pero al crear una _Network Policy_, se restringe todo el tr√°fico menos aquel permitido por la pol√≠tica.

Las policies deben redactarse desde la perspectiva del _pod_ que aplica, para determinar correctamente la direcci√≥n del tr√°fico. Adem√°s, n√≥tese que deben considerarse las ‚Äúdos direcciones‚Äù de tr√°fico, para que la conexi√≥n suceda con √©xito.

Las policies trabajan con la operaci√≥n l√≥gica ‚Äú_uni√≥n_‚Äù, por lo que se combinan al declararse varias.

Al habilitar Ingress traffic, por defecto se habilitan las respuestas a ese tr√°fico (pseudo _egress_).

Algunos rangos _CIDR_ √∫tiles son:

* 10.0.0.0/32: Incluye solo la IP indicada

* 10.0.0.0/24: Incluye el √∫ltimo octeto, o sea 10.0.0.0-10.0.0.255

* 10.0.0.0/16: Incluye los √∫ltimos dos octetos, o sea 10.0.0.0-10.0.255.255

* 0.0.0.0/0: Incluye a toda la red

Nota: El rango 10.0.0.0 es un ejemplo, pero puede ocupar su lugar cualquier IP.

Ejemplo de _Netpol_:

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:¬† name: <nombre> spec:¬† podSelector:¬† ¬† matchLablels:¬† ¬† ¬† label1: value1¬† policyTypes: - Ingress - Egress #Notese cambio de seccion ingress: - from: - ipBlock: cidr: 172.17.0.0/16 except: - 172.17.1.0/24 - namespaceSelector: matchLabels: project: myproject - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379 egress: - to: - ipBlock: cidr: 10.0.0.0/24 ports: - protocol: TCP port: 5978`

N√≥tese que _ports_ est√° al mismo nivel que _from_.

### Habilitar todo el tr√°fico

Para habilitar todo el tr√°fico de un tipo, dejar el _podSelector_ vac√≠o, al igual que la secci√≥n _ingress_ o _egress_ (seg√∫n corresponda):

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-all-ingress spec: podSelector: {} ingress: - {} policyTypes: - Ingress`

### Habilitar todo el tr√°fico para un pod

Omitir secciones _From_ o _To_, con la opci√≥n de declarar un puerto de destino.

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata:¬† name: <nombre> spec:¬† podSelector:¬† ¬† matchLablels:¬† ¬† ¬† label1: value1¬† policyTypes: - Ingress ingress: #Opcional, para un puerto de destino ports: - protocol: TCP port: <Puerto>`

### Habilitar uso de DNS interno

`ports:¬† ¬† - port: 53¬† ¬† ¬† protocol: UDP¬† ¬† - port: 53¬† ¬† ¬† protocol: TCP`

### Denegar todo el tr√°fico

Dejar en blanco el _podSelector_ y declarar un _policyType_, pero omitir las secciones _ingress_/_egress_ por completo:

`apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress spec: podSelector: {} policyTypes: - Ingress`

### Aclaraci√≥n importante sobre los Arrays y los Diccionarios en una NetPol

Siendo que la secci√≥n _from/to_ recibe Items (Array), cada item de la regla (inicia con ‚Äú-‚Äù) es una subregla nueva y entre varias actuan como el operador l√≥gico ‚Äúo‚Äù (‚Äúor‚Äù):

`-podSelector:¬† matchLablels:¬† ¬† label1: value1¬† -namespaceSelector: ¬† matchLablels: ¬† ¬† label1: value1`

En caso de crear dos items en una misma regla, en cambio, el operador l√≥gico ser√≠a ‚Äúy‚Äù (‚Äúand‚Äù):

`-podSelector:¬† matchLablels:¬† ¬† label1: value1¬† namespaceSelector: ¬† matchLablels: ¬† ¬† label1: value1`

##### Comandos de Netwrok Policies

`kubectl get netpol`

Para m√°s informaci√≥n, ver [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/).

![:file_cabinet:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f5c4.png) Almacenamiento
================================================================================================================================================================

Storage en Docker
-----------------

_Docker_ por defecto arma una estructura de carpetas sobre el host, en la ruta _/var/lib/docker_.

Es importante entender adem√°s que esta plataforma trabaja con _capas_ (o _layers_), donde cada capa es una adici√≥n de informaci√≥n o configuraci√≥n sobre la capa inicial, que es la _imagen_; al concebir la _imagen_ final, se convierten las _capas_ en solo lectura. Adem√°s, arma un _cach√©_ con esas capas, para utilizarlas para _im√°genes_ similares (a las cuales les aplica otras capas arriba).

Luego, al encender un _container_ con esa _imagen_, el mismo ocupa una _capa_ m√°s, que es de lectura/escritura. Al trabajar sobre esa capa, la imagen y sus capas quedan intactas.

Tipos de montado hacia un container:

\-**Volume**: Monta un volumen a una ruta.

\-**Bind**: Monta una ruta hacia otra ruta (como _vol√∫men_).

Storage Drivers
---------------

Son controladores que se encargan de negociar y proveer almacenamiento a los _containers_, con hardware (usualmente virtual) del host.

Existen varias opciones, todas de terceros:

* AUFS

* ZFS

* BTRFS

* Overlay

Volume Drivers
--------------

Controladores que administran los montajes de _vol√∫menes_ sobre los _containers_.

Existen varias opciones:

* Local (Default)

* Convoy

* GlusterFS

Container Runtime Interface (CRI)
---------------------------------

Interfaz de _K8s_ que estandariza la interacci√≥n de _contenedores_ con diferentes _motores de runtime_ (plataformas de virtualizaci√≥n de containers). De esta forma, se puede correr _Kubernetes_ con _container runtime_ a elecci√≥n, siempre y cuando sea _CRI-Compliant_.

Runtimes disponibles:

* Docker (Deprecado)

* rkt

* cri-o

* containerd

Luego, los _containers_ compatibles con estos _runtimes_ son aquellos que cumplan con el estandar _OCI_ (_Open Container Initiative_).

[https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/)

Container Storage Interface (CSI)
---------------------------------

Interfaz que estandariza la interacci√≥n de _contenedores_ con diferentes _Storage Drivers_.

La interfaz no es propia de Kubernetes, sino que es un est√°ndar universal.

Volumes (Vol√∫menes)
-------------------

Por defecto est√°n pensados para ser transitorios y solo estar activos durante la duraci√≥n del _container_ (o del _pod_ en caso de _K8s_). Para tener persistencia, se utilizan vol√∫menes externos que se adjuntan al _pod_, definiendo la configuraci√≥n en la secci√≥n _spec_:

`spec:¬† volumes: ¬† - name:¬†<nombreVolumen>¬† hostPath:¬† ¬† ¬† path: /data¬† ¬† ¬† type: Directory`

Luego de definir el _vol√∫men_, debe definirse el _montaje_:

`spec:¬† containers:¬† ¬† - name: container1¬† ¬† ¬† volumeMounts:¬† ¬† ¬† ¬† - name: <nombreVolumen>¬† ¬† ¬† ¬† ¬† mountPath: /etc/mounts/myvolume`

N√≥tese¬† que definir un volumen de esta forma implica un desaf√≠o de administraci√≥n, por lo que es mejor utilizar _vol√∫menes persistentes_ (_PersistentVolume_) y fuera de los discos de los _nodos_.

Existen diferentes tipos de soluciones para alojar el almacenamiento que ocupan los _vol√∫menes_:

* Localhost (por defecto, en el disco de un nodo)

* NFS

* GlusterFS

* Cloud Providers (AWS EBS/Azure Disks/GCP)

* Flocker

* Ceph

###

Persistent Volume

_Vol√∫men_ persistente que luego puede ser reclamado por los _pods_ y cuya existencia no depende de la existencia de estos √∫ltimos.

Esto comprende aprovisionamiento de tipo est√°tico, aunque existe tambi√©n el aprovisionamiento din√°mico (por ejemplo en los _Cloud Providers_).

`apiVersion: v1 kind: PersistentVolume metadata:¬† name: pv-vol1 spec:¬† accessModes:¬† ¬† - ReadWriteOnce¬† capacity:¬† ¬† storage: 1Gi¬† awsElasticBlockStore:¬† ¬† volumeID: <volume-id>¬† ¬† ¬† fsType: ext4 ¬†¬†persistentVolumeReclaimPolicy: Retain`

El campo `persistentVolumeReclaimPolicy` indica qu√© sucede con el _vol√∫men_ luego de que el _pod_ lo libere.

Para utilizar un _PV_ del tipo _hostPath_, donde el volumen se monta sobre un _nodo_ (no recomendado), utilizar esta definici√≥n:

`apiVersion: v1 kind: PersistentVolume metadata: name: pv-analytics spec: capacity: storage: 100Mi accessModes: - ReadWriteMany hostPath: path: /pv/data-analytics`

### Persistent Volume Claim

Es una _petici√≥n de vol√∫men_ con ciertas caracter√≠sticas deseadas, que busca un vol√∫men persistente que las cumpla al menos en el m√≠nimo (en tama√±o puede sobrar espacio, aunque es poco efectivo).

Una vez adquirido el vol√∫men, se asocia el mismo a la petici√≥n y ning√∫n otro _PVC_ (_PersistentVolumeClaim_) puede reclamarlo. Si no encuentra vol√∫menes compatibles, queda como pendiente.

Las caracter√≠sticas que busca cumplir son:

* Sufficient Capacity

* Access Modes

* Volume Modes

* Storage Classes

* Label Selector

Al eliminar un PVC utilizado por un _pod_, queda en estado _Terminating_ hasta que el _pod_ lo libera.

Yaml de ejemplo:

`apiVersion: v1 kind: PersistentVolumeClaim metadata:¬† name: pvc1 spec:¬† accessModes:¬† ¬† - ReadWriteOnce¬† resources:¬† ¬† requests:¬† ¬† ¬† storage: 500Mi`

Para utilizar un _PVC_ en un _pod_:

`spec:¬† containers:¬† ¬† - name: myfrontend¬† ¬† ¬† image: nginx¬† ¬† ¬† volumeMounts:¬† ¬† ¬† - mountPath: "/var/www/html"¬† ¬† ¬† ¬† name: mypd¬† volumes:¬† ¬† - name: mypd¬† ¬† ¬† persistentVolumeClaim:¬† ¬† ¬† ¬† claimName: myclaim`

Al bindearse (asociarse) un _PersistentVolumeClaim_ a un _PersistentVolume_, la definici√≥n del _PV_ en memoria lleva una secci√≥n de _claimRef_ que tiene los datos del _PVC_ asociado:

`claimRef:¬† ¬† ¬† apiVersion: v1¬† ¬† ¬† kind: PersistentVolumeClaim¬† ¬† ¬† name: local-pvc¬† ¬† ¬† namespace: default¬† ¬† ¬† resourceVersion: "1009"¬† ¬† ¬† uid: df37b253-ad40-4216-8288-1157cb1d41f1`

El _PVC_, en cambio, agrega el valor _volumeName_ en la secci√≥n _spec_; ambos cambian de estado.

### Storage Classes

Define una _clase de almacenamiento_ alojada por un proveedor -usualmente cloud-¬† que permite el _aprovisionamiento din√°mico_ de vol√∫menes, permitiendo no tener que definir un _PersistentVolume_ manualmente (se crea igual, pero se forma autom√°tica en base al _PersistentVolumeClaim_).

Se pueden crear varios tipos de _SC_ y utilizarlos como _tiers_ de vol√∫menes en _K8s_.

Yaml de ejemplo:

`apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:¬† name: google-storage provisioner: kubernetes.io/gce-pd volumeBindingMode: WaitForFirstConsumer parameters: ¬†¬†<par√°metrosDelProvider>`

Uso:

`apiVersion: v1 kind: PersistentVolumeClaim metadata:¬† name: pvc1 spec:¬† accessModes:¬† ¬† - ReadWriteOnce¬† storageClassName: google-storage¬† resources:¬† ¬† requests:¬† ¬† ¬† storage: 500Mi`

En el caso de los _cloud providers_, generalmente crean un disco virtual (_PAAS_) del tipo soportado.

Las _clases_ que utilizan el provisioner [kubernetes.io/no-provisioner](http://kubernetes.io/no-provisioner "http://kubernetes.io/no-provisioner") no soportan aprovisionamiento din√°mico.

![:globe_with_meridians:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f310.png) Networking
====================================================================================================================================================================

Comandos de Networking
----------------------

### En Linux

La configuraci√≥n de red de _Kubernetes_ est√° fuertemente inspirada en la configuraci√≥n de red de _Linux_.

Para que los cambios realizados por los comandos sean persistentes, deben agregarse en la configuraci√≥n de red.

* Buscar una cadena de texto en la salida de otro comando: `<comando> | grep <cadena>` o bien `<comando> | grep -i ^<cadena>` para _Regex_.

* Ver procesos de _SO_: `ps -aux`

* Ver IP: `ifconfig` o `ip addr`

* Ver interfaces de red: `ip link`

* Asignar IP: `ip addr add <ip>/24 dev eth0`

* Ver rutas y Gateways configurados: `route`

* Agregar ruta: `ip route add <IPDestino/24 via <IPGateway>`

* Ruta default para cualquier red no conocida:`ip route add default via <IPGateway>`

Como _gateway_ usualmente se utiliza el router.

* Activar forwardeo de paquetes entre interfaces de red:

`#Obtener valor del archivo cat /proc/sys/net/ipv4/ip_forward #Habilitar echo 1 > /proc/sys/net/ipv4/ip_forward #Deshabilitar echo 0 > /proc/sys/net/ipv4/ip_forward`

Es un archivo con valor _booleano_.

* Ver archivo hosts de DNS: `cat /etc/hosts`

* Ver servidores DNS configurados: `cat /etc/resolv.conf`

Ejemplo de archivo _resolv.conf_:

`nameserver <IPServer1> nameserver <IPServer2> nameserver <IPServer3>`

Para agregar un _dominio_ base en los queries DNS, agregar una l√≠nea con formato `search <dominio1> <dominio2>`. De esta forma, todos los nombres _DNS_ se intentar√°n completar con esos dominios (formando un _Fully-Qualified-Domain-Name_ o _FQDN_) en primer lugar y luego se har√° failover con un query DNS normal.

* Ver orden de preferencia para resoluci√≥n DNS:

`#Ver cat /etc/nsswitch.conf #Cambiar a Host file primero hosts: file dns #Cambiar a DNS server primero hosts: dns file`

Por defecto, las consultas van al archivo _Hosts_ en primera instancia y luego al archivo de _DNS_ (_resolv.conf_). La excepci√≥n a la regla es la herramienta **NSLookup**, que utiliza los _Nameservers_ directamente.

* Alternativa a nslookup: `dig <dominio>`

* Forwardear puerto: `iptables -t nat -A PREROUTING --dport <puertoEntrada> --to-destination <ip>:<puertoDestino> -j DNAT`

* Ver IP y MAC de vecino de red: `arp <hostname>`

* Ver estad√≠sticas de red por proceso: `netstat -nplt`

* Ver conexiones establecidas por un proceso: `netstat -anp | grep etcd`

### Trabajar con Namespaces en Linux

Los _Namespaces_ en _Linux_ tambi√©n separan los recursos para un determinado grupo de procesos. Esta es una situaci√≥n deseada para los _containers_, que deben correr aislados de los detalles del _Host_.

En el caso particular de los ejemplos a cotninuaci√≥n, se aislan recursos de red.

* Ejecutar comando dentro de un Namespace de red: `ip netns <Namespace> exec Ccomando>`

* Conectar dos _Namespaces_:

    1. Linkear dos interfaces de red: `ip link add <interfaz1> type veth peer name <interfaz2>`

    2. Asociar cada interfaz a un Namespace: `ip link set <interfaz1> netns <Namespace1>;ip link set <interfaz1> netns <Namespace1>`

    3. Agregar una IP para cada interfaz: `ip -n <namespace1> addr add¬† <ip1> dev <interfaz1>;ip -n <Namespace2> addr add¬† <ip2> dev <interfaz2>`

    4. Encender interfaces: `ip -n <namespace1> link set <interfaz1> up;ip -n <Namespace2> link set <interfaz2> up`

* Conectar m√∫ltiples _Namespaces_:

    1. Crear una interfaz de red en modo _bridge_ (_Linux Bridge_ nativo) para que act√∫e como _Switch_: `ip link add <InterfazSwitch> type bridge`

    2. Encender interfaz switch: `ip link set dev <InterfazSwitch> up`

    3. Por cada Namespace, crear una interfaz para el mismo y otra para su puente (_bridge_) hacia el _Switch_: `ip link add <InterfazNamespace-i> type veth peer name <InterfazNamespace-i-Bridge>`

    4. Por cada Namespace, asignar la interfaz creada al mismo: `ip link set <InterfazNamespace-i> netns <Namespace-i>`

    5. Por cada Namespace, asociar la interfaz bridge creada al extremo del _Switch_, como _Master_: `ip link set <InterfazNamespace-i-Bridge> master` `<InterfazSwitch>`

    6. Por cada Namespace, agregar una direcci√≥n IP: `ip -n <Namespace-i> addr add¬† <IP-i> dev <InterfazNamespace-i>`

    7. Por cada Namespace, encender las interfaces: `ip -n <Namespace-i> link set <InterfazNamespace-i> up`

    8. \-Opcional- Para tener conectividad con el _Host_, agregar una direcci√≥n IP a la interfaz del _Switch_: `ip addr add <IP-Interfaz-Switch> dev <Interfaz-Switch>`

    9. \-Opcional- Para tener conectividad desde la red privada armada al mundo externo, se puede utilizar el _Host_ como _Gateway_. Por cada Namespace ejecutar: `ip netns exec <Namespace-i> ip route add default via <IPLocalHost>`. Adem√°s, activar el _Nateo_ en el _Host_ para que los paquetes se envien en nombre del mismo: `iptables -t nat -A POSTROUTING -s <RangoDeIPNamespace>/<Tama√±o> -j MASQUERADE`

    10. \-Opcional- Para tener conectividad desde el mundo externo a la red privada, armar un forwardeo de puerto del _Host_ hacia la red: `iptables -t nat -A PREROUTING --dport <puertoEntrada> --to-destination <IP-Interfaz-Switch>:<puertoDestino> -j DNAT`

Diagrama del √∫ltimo ejemplo:

Open

### En Docker

* Forwardear puerto a un container: `docker run -p <puertoOrigen>:<PuertoDestino> <container>`

Networking en Docker
--------------------

Pueden crearse _containers_ con redes de tipo:

a-Sin red

b-Compartiendo la red del host (limitado en cuesti√≥n de puertos)

c-_Bridge_: Esta topolog√≠a define una red privada de _Namespaces_ interconectada por un _Switch_, que a su vez tiene conexi√≥n con el _Host_; luego los _containers_ se conectan a cada _Namespace_.

Container Networking Interface (CNI)
------------------------------------

Es un _plugin_ de red que estandariza librer√≠as y formas de conectar _containers_ _Linux_ a la red, de forma _vendor-neutral_. Es un proyecto incubado bajo el ala de _CNCF_ y es utilizado por _Kubernetes_, _CRI-O, Mesos_ y otros.

En general se ejecuta como _script_ o como _binario_ ejecutable y su estandarizaci√≥n hace que sea compatible con diferentes plataformas; cada _vendor_ puede desarrollar su propio _plugin_, pero respetando los est√°ndares preestablecidos.

[https://github.com/containernetworking/cni](https://github.com/containernetworking/cni)

_Network Addons_ disponibles para _K8s_:

* Calico

* Flannel

* Weave Net

Entre otros.

Para m√°s informaci√≥n, ver art√≠culos: [https://kubernetes.io/docs/concepts/cluster-administration/addons/](https://kubernetes.io/docs/concepts/cluster-administration/addons/) y [https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model](https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model).

### Obligaciones del est√°ndar

El est√°ndar _CNI_ implica que un _Network Plugin_ compatible debe cumplir ciertos requisitos:

* Debe soportar comandos ADD/DEL/CHECK

* Debe soportar par√°metros ContainerID, NetworkNS, entre otros

* Debe encargarse de las asignaciones de IPs a los _containers_ o _pods_

* Toda la configuraci√≥n de red debe negociarse en formato _JSON_

_Docker_ utiliza otro est√°ndar llamado _CNM_ (_Container Networking Model_).

Conexi√≥n de containers a la red
-------------------------------

### Forma general

Todas las soluciones de _contenerizaci√≥n_ utilizan la siguiente metodolog√≠a -a gran escala- para conectar containers dentro de un host:

1)Crear _Network Namespace_ (_netns_)

2)Crear _switch virtual_ en modo _Bridge_

3)Crear 2 _Virtual Ethernets_ (_vEths_) para conectar al _netns_

4)Adjuntar _vEth1_ al _netns_

5)Adjuntar _vEth2_ al _Bridge_

6)Asignar direcciones _IP_ a ambas _VE_ths

7)Encender _interfaces_ de red de ambas _VE_ths

8)Habilitar _nateo_ con IP _Masquerade_ mediante la _IP_ del _Host_

### Est√°ndar definido por CNI

1)El _Container Runtime Interface_ (_CRI_) crea un _Network Namespace_

2)Identificar red a conectar al _container_

3)El _CRI_ invoca el _Network Plugin_ cuando se agrega un _container_, con el verbo _add_

3)El _CRI_ invoca _Network Plugin_ cuando se elimina un _container_, con el verbo _del_

Red en Kubernetes
-----------------

### Node Layer

Cada _host_ (_nodo_) debe tener su interfaz de red, IP y hostname √∫nicos. Adem√°s, debe tener una serie de puertos abiertos (para atender los diferentes servicios que aloja).

### Pod Layer

_Kubernetes_ no trae una soluci√≥n built-in de networking en este nivel, sino que hay que integrar con una soluci√≥n de terceros. Para esto -m√°s all√° del _Network Plugin_ utilizado- el est√°ndar _CNI_ act√∫a como _middleman_ e indica qu√© configuraci√≥n debe utilizar _K8s_ para invocar el _plugin_ cada vez que debe conectar un _pod_; esta configuraci√≥n se env√≠a como par√°metro al _Kubelet_ (de cada _nodo_).

#### Requerimientos de modelo de red

\-Todo _pod_ debe tener una IP √∫nica

\-Debe existir comunicaci√≥n de red entre todos los _pods_, sin _nateos_

#### IP Address Assignment Management (IPAM)

Lo √∫nico que requiere _Kubernetes_ en este sentido es que no haya duplicados de _IPs_. La configuraci√≥n _CNI_ tiene una secci√≥n de configuraci√≥n _IPAM_, donde se define la herramienta a la cual delegar la configuraci√≥n _IP_:

`"ipam": { "type":"host-local", "subnet":"<subnet>", ...}`

Las soluciones locales disponibles son:

* Host-Local

* DHCP

Tambi√©n est√° la posibilidad de dejar esta tarea en manos del _Network Plugin_. Por ejemplo, el _plugin_ de _Weaveworks_ utiliza el rango de _IP_ `10.32.0.0/12` (1 millon de IPs).

### Uso de CNI en K8s

Los par√°metros `--network-plugin` (_plugin_ a utilizar), `--cni-bin-dir` (ruta de ejecutables de _plugins_ soportados) y `--cni-conf-dir` (configuraci√≥n _CNI_) indican al _Kubelet_ de cada _nodo_ c√≥mo operar con la red.

Usualmente se encuentran los _binarios_ de los _plugins_ soportados en _/opt/cni/bin_ y la configuraci√≥n en _/etc/cni/net.d_.

### Weave Net CNI (Network Plugin)

Este es un _Network Plugin_ de **Weaveworks**, compatible con _Kubernetes_.

Utiliza un _agente_ en cada nodo (desplegado como _Daemonset_) como conector entre la red principal y el resto de los _nodos_. No solo se encarga de ejecutar el _CNI_ plugin de cada _nodo_ para conectar un _pod_ a la red, sino que adem√°s de estar consciente de la topolog√≠a de red y de qu√© _nodo_ aloja qu√© direcciones _MAC_; de esta forma, logra conectar todos los _pods_

Para su despliegue en _K8s_ necesita adem√°s _Roles_, _ClusterRoles_, una _ServiceAccount_ y los correspondientes _bindings_ a los roles mencionados. Para m√°s detalles sobre c√≥mo instalar, ver art√≠culos: [https://www.weave.works/docs/net/latest/kubernetes/kube-addon/](https://www.weave.works/docs/net/latest/kubernetes/kube-addon/) y [https://www.weave.works/docs/net/latest/kubernetes/](https://www.weave.works/docs/net/latest/kubernetes/).

Service Networking
------------------

Los _service_ (o servicios) de _Kubernetes_ son unidades l√≥gicas y no un _pod_ o una _VM_ en s√≠. Adem√°s, son recursos disponibles _cluster-wide_ y no sobre un solo _nodo_ (pero si son _Namespaced_). El _Kube-APIServer_ le asigna una _IP_ a cada _servicio_, de las disponibles en el rango _CIDR_ declarado con el par√°metro `--service-cluster-ip-range` enviado al encender el componente. Por defecto ocupa el rango 10.0.0.0/24 (10.0.0.0-255) y no debe coincidir con el rango que usan los _pods_ con la soluci√≥n _CNI_.

Los encargados de hacer funcionar la conectividad _service_\-_pod_ (back-end) son los componentes _Kube-Proxy_ de cada _nodo_, que con la _IP_ del _servicio_ arman una regla de _traffic forwarding_ (usando _iptables_ o _ipvs_) para el tr√°fico que llega a la _IP_ del _servicio_ hacia la _IP_ del _pod_ (back-end). Suelen dejar logging del _servicio_ -y por ende de _iptables_\- sobre la ruta `/var/log/kube-proxy.log` (tambi√©n se pueden ver los logs del _pod_).

Open

DNS en Kubernetes
-----------------

### Kube-DNS (default)

Es una capa de las √∫ltimas a configurar en materia de _Networking_, ya que primero tiene que haber conexi√≥n _IP_ entre todos los componentes, seg√∫n par√°metros esperados.

Esta soluci√≥n est√° compuesta por un _pod_ (por el cual vela un _deployment_) que actua como _Nameserver_ y se expone mediante un _service_ llamado _kube-dns_.

Open

### DNS para services

Este componente crea un registro _DNS_ para cada _servicio_, junto con su _IP_ (asignada a nivel _cl√∫ster_ por el _Kube-APIServer_).¬†El registro creado es de tipo _A_ o _AAAA_ y carga la _IP_ del _servicio_, aunque -si el servicio es _Headless_\- puede cargar tambi√©n las _IPs_ de los _Endpoints_ declarados para el _servicio_. Adem√°s, se crea un registro de tipo _SRV_ por cada puerto ‚Äúnombrado‚Äù (_named_) que expone, con el formato `_<NombrePuerto>._<Protocolo>.<ServiceNombre>.<NamespaceNombre>.svc.cluster-domain.example`.

El _FQDN_ de cada servicio tiene el formato `<ServiceNombre>.<NamespaceNombre>.svc.cluster.local`. Esto se debe a que por cada _Namespace_ el componente _Kube-DNS_ crea un _subdominio_ con el nombre del mismo, dentro del cual se registran los records de los _servicios_. Luego a todos los _subdominios_ de _namespaces_ los agrupa dentro de otro _subdominio_ llamado **svc** (`<ServiceNombre>.<NamespaceNombre>.svc`) y a este √∫ltimo _subdominio_ lo agrega al _dominio root_ llamado **cluster.local** (componiendo el FQDN mencionado).

Esto significa que dentro de un mismo _Namespace_, se puede invocar el _servicio_ mediante su nombre y resolver correctamente la _IP_; desde otro _NS_ \-en cambio- debe invocarse con el nombre como subdominio del _NS_ que aloja el servicio `<ServiceNombre>.<NamespaceNombre>`.

Open

[https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)

Para que esto funcione a nivel _pod_ (y a nivel _container_), el _Kubelet_ alimenta la siguiente configuraci√≥n por defecto a cada _pod_ que aloja:

**Archivo resolv.conf**

`nameserver <IPDeSoluci√≥nDNSComoPod> search <namespaceNombre>.svc.cluster.local svc.cluster.local cluster.local options ndots:5`

Los dominios declarados en la secci√≥n _search_ indican los primeros dominios a consultar al recibir un nombre _DNS_ y la opci√≥n de _ndots_ con valor 5 indica que si el nombre tiene menos de 5 puntos, primero consultar√° todos los dominios en _search_, antes de tratarlo como un nombre absoluto (y buscar afuera).

Tambi√©n es configurable a nivel _pod_ el tipo de _DNSPolicy_ y -en caso de desear una configuraci√≥n custom- usar _DNSConfig_ para armar una configuraci√≥n local para el mismo.

### DNS para pods

El uso de _DNS_ para _pods_ no est√° habilitado por defecto. En caso de habilitarlo, utiliza el formato `<xxx-xxx-xxx-xxx>.<namespace>.pod.cluster.local` (_FQDN_ similar al de los _servicios_, pero en el subdominio **pod**, en vez de **svc**), donde el nombre del registro _DNS_ es la IP del pod, reemplazando puntos por guiones.

### Core-DNS

Es un servidor _DNS_ multi-proposito muy flexible, progamado en lenguaje _Go_ y que se incuba bajo el paraguas de _CNCF_. Se utiliza en general como reemplazo de _Kube-DNS_, desplegado como _deployment_ en el _Namespace_ Kube-System, donde -gracias a ciertos roles- administra la resoluci√≥n _DNS_ de los _pods_.

M√°s info. en: [https://coredns.io/](https://coredns.io/) y [https://kubernetes.io/docs/tasks/administer-cluster/coredns/#:~:text=CoreDNS%20is%20a%20flexible%2C%20extensible,as%20the%20Kubernetes%20cluster%20DNS.&text=You%20can%20use%20CoreDNS%20instead,upgrade%20the%20cluster%20for%20you.](https://kubernetes.io/docs/tasks/administer-cluster/coredns/#:~:text=CoreDNS%20is%20a%20flexible%2C%20extensible,as%20the%20Kubernetes%20cluster%20DNS.&text=You%20can%20use%20CoreDNS%20instead,upgrade%20the%20cluster%20for%20you.).

En la versi√≥n 1.21 de _Kubernetes_, la herramienta de despliegue de cl√∫ster _Kubeadm_ directamente utiliza _Core-DNS_ en lugar de la soluci√≥n nativa.

Un archivo de configuraci√≥n -usualmente almacenado en `/etc/coredns/CoreFile`\- indica la configuraci√≥n de la herramienta y debe montar un plugin de _Kubernetes_ para funcionar con el mismo:

`:53 { #Plugins a cargar errors health kubernetes cluster.local in-addr-arpa ip6.arpa { pods insecure #Habilita creaci√≥n de DNS para pods...} prometheus: 9153 proxy: . /etc/resolv.conf #Para consultas DNS que no puede resolver, esto es sobre el nodo master cache 30 reload}`

En _Kubernetes_ este archivo se env√≠a como _configmap_, para poder editarlo y revisarlo.

La resoluci√≥n de _DNS_ que delega sobre los _NameServers_ del host (archivo resolv.conf) debe coincidir con el servicio _DNS_ de _Kubernetes_:

`#Sobre la configuraci√≥n cat /etc/resolv.conf> nameserver <IPDeDNSDeK8s> search default.svc.cluster.local svc.cluster.local cluster.local #Sobre el kubelet cat /var/lib/kubelet/config.yaml clusterDNS: - <IPDeDNSDeK8s>`

Prestar atenci√≥n a la entrada `default.svc.cluster.local`, ya que es gracias a ella que no se deben utilizar _FQDN_ para el _namespace_ llamado _default_.

Ingress
----------

Es una soluci√≥n que permite el ingreso de tr√°fico externo hacia un cl√∫ster de _Kubernetes_, adem√°s de -en general- otorgar la posibilidad de terminaci√≥n de conexiones TLS/SSL y resoluci√≥n basada en nombres de dominio.

Se desprenden dos conceptos:

* **Ingress Resource**: Objeto API de _Kubernetes_, configurable mediante un manifiesto. El mismo declara la ruta del tr√°fico de ingreso externo para un caso particular, declarando _nombre de dominio_, _URL Path_ y _Back-end_ (un _service_ de _Kubernetes_).

* **Ingress Controller**: _Controlador_ externo integrado con el servicio _Ingress_ de _Kubernetes_ que se encarga de proveer y configurar la infraestrucutra de red (usualmente un _LoadBalancer_ y en ocasiones alg√∫n servidor _HTTP_) necesaria para alojar las rutas declaradas en los manifiestos _Ingress_. Este _driver_ analiza los recursos _Ingress_ desplegados y configura su capa _HTTP_ para cumplir con las mismas.

Kubernetes no tiene un _Ingress Controller_ por defecto.

M√°s info. en: [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/).

### Ingress controllers

Algunos de los _IC_ m√°s conocidos son:

* NGINX (Soportado oficialmente)

* TRAEFIK

* HAPROXY

* ISTIO

Un _controlador_ de este tipo se despliega como _deployment_ dentro del cl√∫ster.

#### Annotations

En general los ICs tienen configuraciones extendidas que se envian como _annotations_ sobre el manifiesto _Ingress Route_. Por ejemplo, en el controlador de _NGINX_ (ver art√≠culo [https://kubernetes.github.io/ingress-nginx/examples/](https://kubernetes.github.io/ingress-nginx/examples/)) el par√°metro `-rewrite-target` permite omitir el _URL Path_ enviado al _Ingress_, o bien sobreescribirlo con otro valor, para casos donde el _path_ utilizado por el mismo no coincida con los path mapeados dentro de la aplicaci√≥n (sobreescribe el path con el path declarado en `rules.http.paths.path`).

### Ingress Route

Manifiesto ejemplo:

`apiVersion: networking.k8s.io/v1 kind: Ingress metadata:¬† name: <nombreIng>¬† annotations:¬† ¬† nginx.ingress.kubernetes.io/rewrite-target: / spec:¬† rules: - host:¬†<DominioURL> http:¬† ¬† ¬† paths:¬† ¬† ¬† - path: <Path>¬† ¬† ¬† ¬† pathType: Prefix¬† ¬† ¬† ¬† backend:¬† ¬† ¬† ¬† ¬† service:¬† ¬† ¬† ¬† ¬† ¬† name: <nombreSvc>¬† ¬† ¬† ¬† ¬† ¬† port:¬† ¬† ¬† ¬† ¬† ¬† ¬† number: <Puerto> ¬† name: <nombrePto>`

### Ingress class

Una _clase_ de _Ingress_ define qu√© tipo de _controlador_ _Ingress_ (o mejor dicho, cual) interpreta un manifiesto _Ingress route_ particular. Esto se debe a que pueden existir m√∫ltiples _IC_ conviviendo en un cl√∫ster.

Hasta la versi√≥n 1.18 de _Kubernetes_, extraoficialmente se utilizaba la _annotation_ `kubernetes.io/ingress.class` para definir el controlador a utilizar, pero en esta versi√≥n se defini√≥ un nuevo campo en la secci√≥n `spec` llamada `ingressClassName` que permite definir el controlador a utilizar en un campo oficialmente habilitado para ello.

Para que esto funcione, debe existir una _Ingress Class_ que declara el _controlador_ a utilizar, el nombre de la clase y adem√°s la posibilidad de definirla como default (_annotation_ `ingressclass.kubernetes.io/is-default-class: "true"`).

`apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller name: nginx-example annotations: ingressclass.kubernetes.io/is-default-class: "true" spec: controller: k8s.io/ingress-nginx`

### Caso de Service tipo Load Balancer

Cuando se utiliza un _service_ de tipo _Load Balancer_, el _Cloud Provider_ utiliza su propio servicio _PAAS_ de _Proxy_ y ese est√° configurado para redirigir el tr√°fico a los puertos X de los _nodos_, donde el servicio expone la aplicaci√≥n deseada.

### Comandos Ingress

* Crear Ingress:

`kubectl create ingress <ingress-name> --rule="host/path=service:port"`

![:diamond_shape_with_a_dot_inside:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4a0.png) Dise√±o de un cl√∫ster Kubernetes
==========================================================================================================================================================================================================

Alta disponibilidad (High Availability)
---------------------------------------

Los lineamientos generales de dise√±o para un cl√∫ster de _K8s_ son:

\-Contar con un m√≠nimo de 4 _nodos_

\-No alojar cargas de trabajo en _nodos_ _master_ (para ello utilizar _taints_)

\-Alojar el servicio de _ETCD_ de forma externa y en _HA_

\-Contar con m√∫ltiples _master nodes_ para evitar fallas de servicios _API_

\-Utilizar un _Load Balancer_ para exponer el _Kube-APIServer_ al utilizar m√∫ltiples _control plane nodes_

### HA en componentes Stateful de Kubernetes

Aquellos componentes del sistema que no son _Stateless_ tienden a operar en modo _Active-Standby_ para lograr alta disponibilidad, donde el proceso de _leader-election_ define cual es la instancia de la componente activa y el resto opera en modo _StandBy_. En general, tienden a bloquear un objeto de _Kubernetes_ (como un Endpoint) haciendo una modificaci√≥n y re-iniciar el proceso de _LE_ luego de cierto per√≠odo de tiempo (definido por par√°metros de inicio del binario).

### ETCD en HA

Trabaja con varias instancias de la base de datos, donde una sola instancia actua como lider; luego cualquiera permite operaciones de lectura y las operaciones de escritura las coordina el lider (sincronizando todas las instancias).

Para el proceso de _Leader-election_, utiliza el protocolo _RAFT_, que utiliza un sistema de timers aleatorios, donde el primer nodo en finalizar el timer se autoproclama lider (informando a los dem√°s nodos); luego les envia confirmaciones per√≠odicas indicando que sigue asumiendo dicho rol y -en caso de no recibir esas notificaciones- los nodos restantes inician nuevamente el proceso de LE.

El t√©rmino _quorum_ indica el n√∫mero de nodos m√≠nimo que el cl√∫ster necesita para operar propiamente, que se deduce de la f√≥rmula: Nodos/2 + 1 (parte entera). Como en la mayor√≠a de los cl√∫sters HA, se recomiendan n√∫meros impares de nodos, con un m√≠nimo de 3.

Opciones de despliegue
----------------------

\-**Turnkey solutions**: Soluciones que facilitan el despliegue de un cl√∫ster _K8s_, donde el mantenimiento y aprovisionamiento del cl√∫ster (y sus _Virtual Machines_) queda a cargo del usuario. Opciones conocidas: Open-Shift, Cloud Foundry CR y Vagrant.

\-**Hosted solutions**: Soluciones _Kubernetes-As-A-Service_ que brindan _cloud providers_ donde pr√°cticamente todo el mantenimiento queda a cargo del proveedor. Opciones conocidas: GKE, Open Shift Online, AKS y EKS.

### Creaci√≥n manual de un cl√∫ster (Hard Way)

En este tipo de configuraci√≥n se instalan los componentes como _binarios_ en los _hosts_ (o _nodes_) y se los configura como servicios _Systemd_ de _Linux_. Luego uno de los _master nodes_ es el _Certification Authority_ para _TLS_ y todos los componentes se comunican con sus propios pares de llaves _TLS_, con encripci√≥n punta a punta.

Sin entrar en detalles, se realizan los siguientes pasos:

1. Disponer infraestructura con -al menos- 1 _master_ y 1 _worker nodes_ (2 y 2 para tener alta disponibilidad)

2. Definir si el cl√∫ster _ETCD_ se monta con su propia infraestructura o en modo _stacked_ (instalado sobre el mismo cl√∫ster)

3. Elegir un _master node_ como _Certification Authority_, emitir su par de llaves _TLS_, auto-firmar (self-sign) su certificado p√∫blico y guardarlos

4. Emitir pares de certificados cliente -y firmar con el _CA_\- para todos los componentes y usuarios:

    1. _Kube-APIServer_

    2. _Kube-Scheduler_

    3. _Kube-ControllerManager_

    4. _ETCD_

    5. _Service Accounts_ (Utilizado por _Kube-Controller-Manager_ para autorizar los _tokens_ de otros _SA_)

    6. Usuario Admin

5. Repartir certificados:

    1. _Master nodes_: _CA_ (solo p√∫blico), _Kube-APIServer_, _Kube-Scheduler_, _Kube-ControllerManager_, _ETCD_ (si es _stacked_), _Service Accounts_, Usuario Admin

    2. _Worker nodes_: _CA_ (solo p√∫blico)

6. Descargar e instalar _Kubectl_ en los _Master nodes_

7. Crear archivos de configuracion _.kubeconfig_ con _Kubectl_ para los diferentes componentes y usuarios:

    1. _Kube-APIServer_

    2. _Kube-Scheduler_

    3. _Kube-ControllerManager_

    4. Usuario Admin

8. Instalar _ETCD_ (solo si es _Stacked_ topology)

9. Descargar e instalar _binarios_ de _Kube-APIServer_, _Kube-Scheduler_ y _Kube-ControllerManager_

10. Crear archivos de servicio _Systemd_ -y habilitar- para esos componentes de sistema, utilizando los certificados necesarios (_CA_ y propios de cada uno) y los archivos de configuraci√≥n _kubeconfig_ creados previamente. Estos se envian como par√°metros en la l√≠nea de ejecuci√≥n del _binario_

11. Conectar al cl√∫ster con la configuraci√≥n de Admin

12. Si el cl√∫ster fuera _HA_, configurar un _LoadBalancer_ a gusto para distribuir la carga a los _Master nodes_ (https://<IP>:6443)

13. Crear y firmar certificados para los _nodos worker_ (para el _Kubelet_)

14. Crear archivos de configuraci√≥n _kubeconfig_ para _Kubelet_ y _KubeProxy_

15. Descargar e instalar _Container Runtime_, _Kubelet_ y _KubeProxy_ en los _worker nodes_

16. Crear servicios _Systemd_ de _Kubelet_ y _KubeProxy_, utilizando configuraciones _TLS_ (certificados) y _K8s_ (_kubeconfig_) necesarias

17. Conectar al cl√∫ster y verificar existencia del _nodo_

18. Configurar soluci√≥n de _Networking_ para _pods_ (ejemplo Weave)

19. Configurar soluci√≥n _DNS_ (ejemplo CoreDNS)

Open

### TLS Bootstrapping

Es una metodolog√≠a de administraci√≥n de certificados _TLS_ para los _nodos worker_ de _Kubernetes_. Permite que los _nodos_ se auto-generen sus propios pares de llaves _TLS_, hagan un CertificateSigningRequest para firmarlos con el _CA_ (que luego debe aprobar un administrador), los implementen en sus configuraciones _kubeconfig_ y se unan al cl√∫ster nuevamente. Pr√°cticamente permite la auto-administraci√≥n de los _nodos workers_ y su autenticaci√≥n contra el cl√∫ster; esto lo hace aprovechando la _Certification-API_ de _K8s_.

Para ello se crea un _Bootstrap token_¬†asociado al _grupo_ `system:bootstrappers` y luego los _roles_ llamados `system:node-bootstrappers` y `system:certificates.k8s.io:certificatesigningrequests:nodeclient`asociados a dicho _grupo_. Todo esto se realiza con _ClusterRoleBindings_ sobre _roles_ nativos de _Kubernetes_.

Cuando el _nodo_ se une al cl√∫ster, ya no necesita el _bootstrap token_ y empieza a formar parte del grupo `system:nodes`, por lo que se debe asociar el rol `system:certificates.k8s.io:certificatesigningrequests:selfnodeclient` al mismo. para que pueda auto renovar el _CSR_ cuando vence el _certificado_ actual.

Para que funcione, se env√≠a con el par√°metro `--bootstrap-kubeconfig` al _Kubelet_ al iniciarlo un archivo de configuraci√≥n _bootstrap-kubeconfig_ que -en vez de usar un _certificado_\- utiliza un _bootstrap token_. Adem√°s, los par√°metros `--rotate-certificates` y `--rotate-server-certificates` activan la rotaci√≥n de certificados, con la salvedad de que las _CSR_ de _certificados_ de servidor (no los de cliente) requieren de aprobaci√≥n manual.

N√≥tese que el archivo `bootstrap-kubeconfig` reemplaza el archivo `kubeconfig`; de hecho, el _Kubelet_ busca la existencia del primero, en ausencia del segundo. Es luego de hacer el proceso inicial de _bootstrapping_ que el _Kubelet_ confecciona su propio archivo _kubeconfig_ con el certificado recibido.

### Creaci√≥n de un cl√∫ster con Kubeadm

_Kubeadm_ es una herramienta oficial de _Kubernetes_ para facilitar la creaci√≥n y mantenci√≥n de cl√∫sters _K8s_.

Pasos a seguir:

1. Ver validaciones necesarias (art√≠culo [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)):

    1. Requisitos de hardware m√≠nimos: 2 CPU y 2GB de RAM

    2. SO: Linux compatible (Debian o Red Hat y derivados)

    3. Red: Conectividad full, unicidad de hostnames/IPs/MACs/ProductIds y ciertos puertos abiertos

    4. Memoria SWAP deshabilitada

2. Configurar _IPTables_ para que vea tr√°fico de _bridge_

3. Instalar _container runtime_

4. Instalar _Kubeadm_, _Kubectl_ y _Kubelet_

5. Inicializar _control plane node_ (`kubeadm init`)

6. Guardar datos de comando `Join` que muestra _kubeadm_

7. Instalar soluci√≥n de _pod networking_ como _deployment_

8. Utilizar _taints_ en el _control plane_ para aislarlo de cargas de trabajo

9. Unir _nodos worker_ al cl√∫ster (`kubeadm join`)

10. Testear integridad del cl√∫ster

11. Instalar soluci√≥n de _Service DNS_ como _deployment_

Utiliza _Kube-Scheduler_, Kube-Controller-Manager, _Kube-APIServer_ y _ETCD_ como _static pods_ dentro de _K8s_, gracias al _Kubelet_ (primer componente instalado en conjunto con el _CRI_).

![:question:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/2753.png) Troubleshooting
============================================================================================================================================================

Al diagnosticar problemas en un cl√∫ster _K8s_ es necesario analizar por capas.

Application
-----------

\-Ver que la _app_ sea accesible desde afuera (_Ingress_ o _service nodePort_)

\-Ver que el _servicio_ tenga _endpoints_

\-Comparar _labels_ de _servicio_ y _deployment_ (o _pod_)

\-Revisar el _pod_ con comandos `kubectl get -o yaml` y `kubectl describe` que muestran enfoques diferentes (enfocarse principalmente en _variables de entorno_ y _vol√∫menes_)

\-Revisar _logs_ del _pod_ (utilizando el par√°metro `-f` para hacer un _tail_ en vivo y `--previous` para ver logs de _pods_ anteriores)

\-Aplicar el mismo tipo de troubleshooting sobre otros _pods_ que componen la soluci√≥n, o bien componentes externos con los que interact√∫a la soluci√≥n

Control Plane
-------------

\-Ver estado de los _nodos_

\-Ver estado de los componentes de sistema (ya sea como servicios _Linux_ o como _deployments_ en _K8s_ -o sea sus _static pod manifests_\-)

\-Ver _logs_ del _Kube-APIServer_ (_journalctl_ o _kubectl get logs_)

Worker Node
-----------

\-Ver estado de los _nodos_ (y utilizar el comando `describe`, analizado estado de banderas de recursos esperados):

Open

\-Ver estado de la _VirtualMachine_ del _nodo_ en s√≠

\-Revisar estado del servicio _Kubelet_

\-Ver logs del servicio _Kubelet_ (_journalctl_)

\-Verificar que los certficados no est√©n vencidos, est√©n emitidos para los _DN_ correctos y emitidos por el _CA_ correcto

Networking
----------

\-Identificar _CNI network plugin_ en uso y ver su estado

\-Ver estado de _CoreDNS_ (corre como _deployment_, con un _rol_ asociado, un _Configmap_ y un _servicio_). Si existen pods de _CoreDNS_ que entran en _CrashLoopBackOff_, puede haber un problema de _DNS loop_ por una configuracion `resolv.conf` inv√°lida. Se puede probar:

\-Revisar dicha configuraci√≥n en el _CoreFile_ (config de _CoreDNS_) y ver que apunte a un archivo v√°lido (normalmente `/run/systemd/resolve/resolv.conf`).

\-Agregar `resolvConf: /run/systemd/resolve/resolv.conf` (o un archivo v√°lido) al archivo de configuraci√≥n (_manifiesto_) de los _Kubelet_

\-Deshabilitar cach√© _DNS_ local en _nodos_ y restaurar `resolv.conf` a su config original.

\-Reemplazar el _forwarding_ del archivo _CoreFile_ por el servidor _DNS_ utilizado hacia afuera (ejemplo 8.8.8.8)

\-Ver estado de _Kube-Proxy_ (puede correr como _servicio_ de _Linux_ o como _Daemonset_ de _K8s_). Si falla _Kube-Proxy_, ver:

\-Que el _ConfigMap_ tenga el archivo de configuraci√≥n (`.conf`) y de conexi√≥n al cl√∫ster (_kubeconfig_). Adem√°s, verificar que sean v√°lidos.

\-Ver logs

![:scroll:](https://pf-emoji-service--cdn.us-east-1.prod.public.atl-paas.net/standard/caa27a19-fc09-4452-b2b4-a301552fd69c/64x64/1f4dc.png) Comandos (Kubectl)
==============================================================================================================================================================

### Banderas/flags o par√°metros

#### Correr en modo simulaci√≥n

`--dry-run=client`

#### Cambiar formato de salida

`-o yaml/json`

#### Selector por Labels

`--selector label=value`

O bien

`-l label=value`

### Generales

#### Obtener info. del cluster

`kubectl cluster-info`

#### Ver configuraci√≥n actual

`kubectl config-view`

#### Identificar perfil de configuraci√≥n en uso

`kubectl config current-context`

#### Ver objetos

`kubectl get nodes/deployments/pods/services/namespaces/replicasets`

El par√°metro ‚Äú-o wide‚Äù muestr√° m√°s info, incluida la IP.

Existen abreviaciones como ‚Äúsvc‚Äù para ‚Äúservices‚Äù y ‚Äúns‚Äù para ‚Äúnamespaces‚Äù.

Se pueden ver varios tipos de objeto a la vez, numerandolos con coma: kubectl get pod,svc

El par√°metro ‚Äú-l <label>‚Äù permite filtrar por label.

#### Ver eventos

`kubectl get events`

#### Desplegar con un manifiesto YAML

`kubectl apply -f <rutaDeManifiesto>`

Para desplegar la carpeta actual: `kubectl apply -f .`

#### Editar un recurso desplegado en memoria

`kubectl edit <tipoDeRecurso>/<NombreDeRecurso>`

Editar campos no editables de un recurso y guardar de todos modos genera un archivo temporal _YAML_ en la carpeta donde est√° ubicada la consola (PWD), √∫til para re-desplegar con el mismo.

### Cargas de trabajo

#### Crear pod

`kubectl run <nombre> --image <imagen> --port <puerto>`

Los flags `--selector` y `--expose` se pueden agregar para taggear o exponer con servicio, respectivamente.

Para incluir variables de entorno, utilizar `--env:"nombre=valor"` y para incluir un comando `--command -- <comando> <argumento/s>` (debe ser lo √∫ltimo del comando).

`kubectl run <nombre> --image <imagen> --env=‚Äùname=alpha‚Äù --command -- <comando> <argumento>`

Para asignar una _ServiceAccount_, agregar el par√°metro `--serviceaccount <Nombre>`.

#### Exponer un pod

`kubectl expose po <nombrePod> --port <puertoServicio> --selector "label1=value1,label2=value2" --name <nombreServicio> --type ClusterIP`

Luego omitir el par√°metro `--target-port` implica que el n√∫mero de puerto indicado en `--port` (mandatorio) ser√° el utilizado tambi√©n para el _targetPort_. Tambi√©n se puede agregar el protocolo con `--portocol` y valores _TCP_/_UDP_.

#### Crear deployment

`kubectl create deployment <nombreDeployment> --image=<id/url de imagen>`

#### Conectarse a Bash de un POD

`kubectl exec -ti <idDePod> -- /bin/bash`

#### Crear servicio (exponer pods)

`kubectl expose deployment/<nombreDeployment> --type="NodePort" --port <puerto> #Luego para ver el puerto expuesto kubectl describe service <nombreDeployment> #Para conectar curl http://<ip del cluster>:<puertoExpuesto>`

#### Poner un label a un pod

`kubectl label pod <podName> <key:value>`

#### Escalar un deployment

`kubectl scale deployments/<NombreDeployment> --replicas=<n√∫meroDeReplicas>`

#### Actualizar un despliegue

`kubectl set image deployments/<nombreDeployment> <nombreDeployment>=<repositorio>/<imagen> #Seguimiento kubectl rollout status deployments/<nombreDeployment>`

#### Hacer un rollback de despliegue

`kubectl rollout undo deployments/<nombreDeployment>`

#### Cambiar imagen de despliegue

`kubectl set image <deployment> <nombreContainer>=<nombreImagen>`

#### Forwardear puerto local a Kubernetes

`kubectl port-forward service/<servicio> <PuertoLocal>:<PuertoDestino>`

Tambi√©n sirve con un pod.

#### Reiniciar un deployment sin re-deployar

`#Apagar kubectl scale deployment <Deployment> --replicas=0 #Encender kubectl scale deployment <Deployment> --replicas=<CantidadPodsDeseada> #O bien kubectl rollout restart <deployment>`

#### Desplegar con un manifiesto YAML

`kubectl apply -f <rutaDeManifiesto>`

#### Invocar un servicio por FQDN

nombre\_servicio.namespace.svc

`#Ejemplo ping prometheus-k8s.monitoring.svc`

#### Ver logs de un pod

`kubectl logs <pod>`

El par√°metro `-f` permite hacer un _Tail_ y ver el log en vivo.

### Otros

### Obtener info de recursos API

`kubectl api-resources`

#### Debugear DNS

[https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)

`#Implementar pod de debugging de DNS kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml #Ver si resuelve el default DNS service kubectl exec -ti dnsutils -- nslookup kubernetes.default #Ver si esta bien mapeado el nameserver local kubectl exec -ti dnsutils -- cat /etc/resolv.conf #Ver si estan corriendo los pods de kube-dns kubectl get pods -n kube-system -l k8s-app=kube-dns #Ver si hay logs de error kubectl logs -n kube-system -l k8s-app=kube-dns #Ver si existe el servicio kubectl get svc -n kube-system`

#### Crear Config-Map

`kubectl create configmap <name> --from-file=<pathfile1> --from-file=<pathfile2> #O bien kubeclt create configmap <name> --from-literal=<NombreValor1>=<Valor1>`

Luego invocar como Volume Mount y Volume Path en el deployment.

#### Crear secreto

`kubectl create secret generic <nombre> --from-file/literal`

#### Esperar una condicion

`kubectl wait --for=condition=Ready -l=<LabelsDePod> #O bien kubectl wait --for=<Verbo> -l=<LabelsDePod>`

Par√°metro `--timeout` para poner un deadline.
